<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on My New Hugo Site</title>
    <link>http://localhost:1313/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ollama AI Model Server</title>
      <link>http://localhost:1313/pages/projects/ollama/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/projects/ollama/</guid>
      
      <description>&lt;h1 id=&#34;ollama-ai-model-server&#34;&gt;Ollama AI Model Server&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Ollama is a local AI model server deployed in the Fako cluster that provides a unified API for running large language models (LLMs). It runs on GPU acceleration and manages multiple AI models, making them available for various applications including Home Assistant integrations, coding assistants, and general AI tasks.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU Acceleration&lt;/strong&gt;: Optimized for RTX 5070 with 12GB VRAM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Management&lt;/strong&gt;: Automatic downloading and lifecycle management of models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiple Model Support&lt;/strong&gt;: Hosts various models from small (0.5B) to large (32B+)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RESTful API&lt;/strong&gt;: Compatible with OpenAI-style APIs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Persistent Storage&lt;/strong&gt;: Models cached to avoid re-downloading&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto-scaling&lt;/strong&gt;: Memory management with model unloading&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica GPU deployment pinned to the &lt;code&gt;yeezyai&lt;/code&gt; node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Services&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;ClusterIP service on port 11434&lt;/li&gt;
&lt;li&gt;NodePort service for external access&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PersistentVolumeClaim for model storage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMaps&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;General configuration&lt;/li&gt;
&lt;li&gt;GPU-specific settings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sidecar Container&lt;/strong&gt;: Model manager for automatic model downloads&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: 1 NVIDIA GPU (RTX 5070)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 12Gi (request), 36Gi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 4 cores (request), 8 cores (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Persistent volume for model storage&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;general-settings-ollama-configmap&#34;&gt;General Settings (ollama-configmap)&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Default&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_HOST&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Bind address for the API server&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_ORIGINS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;*&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;CORS origins allowed&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_MODELS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;/root/.ollama/models&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Model storage directory&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_DEBUG&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Enable debug logging&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_NOHISTORY&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Keep conversation history&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;gpu-settings-ollama-gpu-configmap&#34;&gt;GPU Settings (ollama-gpu-configmap)&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Default&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_GPU_LAYERS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;999&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Force all layers to GPU&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_GPU_MEMORY&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;12G&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;GPU memory allocation&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_NUM_PARALLEL&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;2&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Concurrent request handling&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_NUM_THREAD&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;8&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;CPU threads for operations&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_MAX_LOADED_MODELS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Models kept in GPU memory&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_KEEP_ALIVE&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;5m&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Model keep-alive duration&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;available-models&#34;&gt;Available Models&lt;/h2&gt;
&lt;p&gt;The model manager automatically downloads and maintains these models:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Running Local AI: My Experience with Ollama and GPU Acceleration</title>
      <link>http://localhost:1313/posts/local-ai-with-ollama/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/local-ai-with-ollama/</guid>
      
      <description>&lt;p&gt;One of the most exciting aspects of the Fako Cluster is the ability to run large language models locally. With privacy concerns around cloud AI services and the desire for unlimited API usage, I decided to dive deep into self-hosting LLMs using Ollama. Here&amp;rsquo;s what I&amp;rsquo;ve learned.&lt;/p&gt;
&lt;h2 id=&#34;the-hardware-foundation&#34;&gt;The Hardware Foundation&lt;/h2&gt;
&lt;p&gt;Running LLMs locally requires serious GPU power. My setup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: NVIDIA RTX 5070 with 12GB VRAM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Node&lt;/strong&gt;: Dedicated &lt;code&gt;yeezyai&lt;/code&gt; node in the cluster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 36GB RAM allocated to the Ollama pod&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This configuration allows me to run models up to 32B parameters, though performance varies significantly based on model size.&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
