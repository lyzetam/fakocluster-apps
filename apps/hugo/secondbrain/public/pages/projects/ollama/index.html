<!DOCTYPE html>
<html lang="en-us"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Ollama AI Model Server | My New Hugo Site</title>
<meta property="og:title" content="Ollama AI Model Server | My New Hugo Site" />
<meta name="twitter:title" content="Ollama AI Model Server | My New Hugo Site" />
<meta itemprop="name" content="Ollama AI Model Server | My New Hugo Site" />
<meta name="application-name" content="Ollama AI Model Server | My New Hugo Site" />
<meta property="og:site_name" content="" />

<meta name="description" content="Local AI model server with GPU acceleration for running large language models">
<meta itemprop="description" content="Local AI model server with GPU acceleration for running large language models" />
<meta property="og:description" content="Local AI model server with GPU acceleration for running large language models" />
<meta name="twitter:description" content="Local AI model server with GPU acceleration for running large language models" />

<meta property="og:locale" content="en-us" />
<meta name="language" content="en-us" />

  <link rel="alternate" hreflang="en" href="http://localhost:1313/pages/projects/ollama/" title="" />






<meta name="generator" content="Hugo 0.148.2">

    
    <meta property="og:url" content="http://localhost:1313/pages/projects/ollama/">
  <meta property="og:site_name" content="My New Hugo Site">
  <meta property="og:title" content="Ollama AI Model Server">
  <meta property="og:description" content="Local AI model server with GPU acceleration for running large language models">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="pages">
    <meta property="article:published_time" content="2025-01-07T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-07T00:00:00+00:00">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="Machine Learning">
    <meta property="article:tag" content="GPU">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Kubernetes">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Ollama AI Model Server">
  <meta name="twitter:description" content="Local AI model server with GPU acceleration for running large language models">


    

    <link rel="canonical" href="http://localhost:1313/pages/projects/ollama/">
    <link href="/style.min.e390ba7da26222f4dc42a349955d76dbbe44e5ce2535a43de5a70633a0a9ec3c.css" rel="stylesheet">
    <link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/favicon.ico">




<link rel="manifest" href="http://localhost:1313/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/icons/favicon.svg">

    
    
</head>
<body data-theme = "" class="notransition">

<script src="/js/theme.js"></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="http://localhost:1313/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title></title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="/pages/about/">
                        About
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/pages/projects/">
                        Projects
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/secondbrain/">
                        Second Brain
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Ollama AI Model Server</h1>
                
                
                
                <div class="post-meta">
                    <time datetime="2025-01-07T00:00:00&#43;00:00" itemprop="datePublished"> Jan 7, 2025 </time>
                </div>
                
            </header>
            
            <div class="page-content">
                <h1 id="ollama-ai-model-server">Ollama AI Model Server</h1>
<h2 id="overview">Overview</h2>
<p>Ollama is a local AI model server deployed in the Fako cluster that provides a unified API for running large language models (LLMs). It runs on GPU acceleration and manages multiple AI models, making them available for various applications including Home Assistant integrations, coding assistants, and general AI tasks.</p>
<h2 id="key-features">Key Features</h2>
<ul>
<li><strong>GPU Acceleration</strong>: Optimized for RTX 5070 with 12GB VRAM</li>
<li><strong>Model Management</strong>: Automatic downloading and lifecycle management of models</li>
<li><strong>Multiple Model Support</strong>: Hosts various models from small (0.5B) to large (32B+)</li>
<li><strong>RESTful API</strong>: Compatible with OpenAI-style APIs</li>
<li><strong>Persistent Storage</strong>: Models cached to avoid re-downloading</li>
<li><strong>Auto-scaling</strong>: Memory management with model unloading</li>
</ul>
<h2 id="architecture">Architecture</h2>
<h3 id="components">Components</h3>
<ol>
<li><strong>Deployment</strong>: Single-replica GPU deployment pinned to the <code>yeezyai</code> node</li>
<li><strong>Services</strong>:
<ul>
<li>ClusterIP service on port 11434</li>
<li>NodePort service for external access</li>
</ul>
</li>
<li><strong>Storage</strong>: PersistentVolumeClaim for model storage</li>
<li><strong>ConfigMaps</strong>:
<ul>
<li>General configuration</li>
<li>GPU-specific settings</li>
</ul>
</li>
<li><strong>Sidecar Container</strong>: Model manager for automatic model downloads</li>
</ol>
<h3 id="resource-requirements">Resource Requirements</h3>
<ul>
<li><strong>GPU</strong>: 1 NVIDIA GPU (RTX 5070)</li>
<li><strong>Memory</strong>: 12Gi (request), 36Gi (limit)</li>
<li><strong>CPU</strong>: 4 cores (request), 8 cores (limit)</li>
<li><strong>Storage</strong>: Persistent volume for model storage</li>
</ul>
<h2 id="configuration">Configuration</h2>
<h3 id="general-settings-ollama-configmap">General Settings (ollama-configmap)</h3>
<table>
  <thead>
      <tr>
          <th>Parameter</th>
          <th>Default</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>OLLAMA_HOST</code></td>
          <td><code>0.0.0.0</code></td>
          <td>Bind address for the API server</td>
      </tr>
      <tr>
          <td><code>OLLAMA_ORIGINS</code></td>
          <td><code>*</code></td>
          <td>CORS origins allowed</td>
      </tr>
      <tr>
          <td><code>OLLAMA_MODELS</code></td>
          <td><code>/root/.ollama/models</code></td>
          <td>Model storage directory</td>
      </tr>
      <tr>
          <td><code>OLLAMA_DEBUG</code></td>
          <td><code>false</code></td>
          <td>Enable debug logging</td>
      </tr>
      <tr>
          <td><code>OLLAMA_NOHISTORY</code></td>
          <td><code>false</code></td>
          <td>Keep conversation history</td>
      </tr>
  </tbody>
</table>
<h3 id="gpu-settings-ollama-gpu-configmap">GPU Settings (ollama-gpu-configmap)</h3>
<table>
  <thead>
      <tr>
          <th>Parameter</th>
          <th>Default</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>OLLAMA_GPU_LAYERS</code></td>
          <td><code>999</code></td>
          <td>Force all layers to GPU</td>
      </tr>
      <tr>
          <td><code>OLLAMA_GPU_MEMORY</code></td>
          <td><code>12G</code></td>
          <td>GPU memory allocation</td>
      </tr>
      <tr>
          <td><code>OLLAMA_NUM_PARALLEL</code></td>
          <td><code>2</code></td>
          <td>Concurrent request handling</td>
      </tr>
      <tr>
          <td><code>OLLAMA_NUM_THREAD</code></td>
          <td><code>8</code></td>
          <td>CPU threads for operations</td>
      </tr>
      <tr>
          <td><code>OLLAMA_MAX_LOADED_MODELS</code></td>
          <td><code>1</code></td>
          <td>Models kept in GPU memory</td>
      </tr>
      <tr>
          <td><code>OLLAMA_KEEP_ALIVE</code></td>
          <td><code>5m</code></td>
          <td>Model keep-alive duration</td>
      </tr>
  </tbody>
</table>
<h2 id="available-models">Available Models</h2>
<p>The model manager automatically downloads and maintains these models:</p>
<h3 id="small-models-05b---4b">Small Models (0.5B - 4B)</h3>
<ul>
<li><strong>qwen2.5:0.5b</strong> - Ultra-lightweight model</li>
<li><strong>llama3.2:3b</strong> - Efficient general-purpose model</li>
<li><strong>phi3:mini</strong> - Microsoft&rsquo;s compact model</li>
</ul>
<h3 id="medium-models-6b---8b">Medium Models (6B - 8B)</h3>
<ul>
<li><strong>qwen3:8b</strong> - Home Assistant specialized</li>
<li><strong>llama3.1:8b</strong> - Latest Llama model</li>
<li><strong>deepseek-coder:6.7b-instruct</strong> - Code-focused model</li>
</ul>
<h3 id="large-models-13b---32b">Large Models (13B - 32B)</h3>
<ul>
<li><strong>deepseek-r1:14b</strong> - Enhanced for autonomous agents</li>
<li><strong>ishumilin/deepseek-r1-coder-tools:14b</strong> - Optimized for Cline</li>
<li><strong>qwen:32b</strong> - Large general-purpose model</li>
</ul>
<h2 id="usage">Usage</h2>
<h3 id="accessing-the-service">Accessing the Service</h3>
<p>Within the cluster:</p>
<pre tabindex="0"><code>http://ollama-gpu.ollama.svc.cluster.local:11434
</code></pre><h3 id="api-examples">API Examples</h3>
<h4 id="list-available-models">List Available Models</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://ollama-gpu.ollama.svc.cluster.local:11434/api/tags
</span></span></code></pre></div><h4 id="generate-text">Generate Text</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -X POST http://ollama-gpu.ollama.svc.cluster.local:11434/api/generate <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;llama3.1:8b&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;prompt&#34;: &#34;Explain kubernetes in simple terms&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><h4 id="chat-completion-openai-compatible">Chat Completion (OpenAI Compatible)</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -X POST http://ollama-gpu.ollama.svc.cluster.local:11434/v1/chat/completions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;llama3.1:8b&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;messages&#34;: [
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">      {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello, how are you?&#34;}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    ]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><h3 id="integration-with-applications">Integration with Applications</h3>
<h4 id="clinevs-code">Cline/VS Code</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;api_provider&#34;</span>: <span style="color:#e6db74">&#34;ollama&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;ollama_base_url&#34;</span>: <span style="color:#e6db74">&#34;http://ollama-gpu.ollama.svc.cluster.local:11434&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;ollama_model_id&#34;</span>: <span style="color:#e6db74">&#34;deepseek-r1:14b&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h4 id="home-assistant">Home Assistant</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># In configuration.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">openai_conversation</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;Local Ollama&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">api_key</span>: <span style="color:#e6db74">&#34;dummy&#34;</span>  <span style="color:#75715e"># Ollama doesn&#39;t need API key</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">base_url</span>: <span style="color:#e6db74">&#34;http://ollama-gpu.ollama.svc.cluster.local:11434/v1&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">model</span>: <span style="color:#e6db74">&#34;qwen3:8b&#34;</span>
</span></span></code></pre></div><h2 id="operations">Operations</h2>
<h3 id="checking-service-status">Checking Service Status</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Check pod status</span>
</span></span><span style="display:flex;"><span>kubectl get pods -n ollama
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># View logs</span>
</span></span><span style="display:flex;"><span>kubectl logs -n ollama deployment/ollama-gpu
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Check model manager logs</span>
</span></span><span style="display:flex;"><span>kubectl logs -n ollama deployment/ollama-gpu -c model-manager
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Monitor GPU usage</span>
</span></span><span style="display:flex;"><span>kubectl exec -n ollama deployment/ollama-gpu -- nvidia-smi
</span></span></code></pre></div><h3 id="managing-models">Managing Models</h3>
<h4 id="list-installed-models">List Installed Models</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl exec -n ollama deployment/ollama-gpu -- ollama list
</span></span></code></pre></div><h4 id="pull-a-new-model">Pull a New Model</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl exec -n ollama deployment/ollama-gpu -- ollama pull model-name
</span></span></code></pre></div><h4 id="remove-a-model">Remove a Model</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl exec -n ollama deployment/ollama-gpu -- ollama rm model-name
</span></span></code></pre></div><h3 id="model-storage-management">Model Storage Management</h3>
<p>Models are stored at <code>/root/.ollama/models</code>. Storage usage varies by model:</p>
<ul>
<li><strong>Small models (0.5B-3B)</strong>: 1-3 GB each</li>
<li><strong>Medium models (6B-8B)</strong>: 4-8 GB each</li>
<li><strong>Large models (13B-32B)</strong>: 8-20 GB each</li>
</ul>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="pod-not-starting">Pod Not Starting</h3>
<ol>
<li><strong>Check GPU availability</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl describe node yeezyai | grep -A10 <span style="color:#e6db74">&#34;Allocated resources&#34;</span>
</span></span></code></pre></div><ol start="2">
<li><strong>Verify GPU runtime</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl get runtimeclass nvidia
</span></span></code></pre></div><ol start="3">
<li><strong>Check events</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl describe pod -n ollama -l app<span style="color:#f92672">=</span>ollama-gpu
</span></span></code></pre></div><h3 id="out-of-memory-errors">Out of Memory Errors</h3>
<ol>
<li>
<p><strong>Reduce loaded models</strong>:</p>
<ul>
<li>Set <code>OLLAMA_MAX_LOADED_MODELS</code> to <code>1</code></li>
<li>Decrease <code>OLLAMA_KEEP_ALIVE</code> duration</li>
</ul>
</li>
<li>
<p><strong>Use quantized models</strong>:</p>
<ul>
<li>Choose models with <code>:q4_0</code> or <code>:q4_K_M</code> tags</li>
</ul>
</li>
<li>
<p><strong>Monitor GPU memory</strong>:</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl exec -n ollama deployment/ollama-gpu -- nvidia-smi
</span></span></code></pre></div><h3 id="slow-response-times">Slow Response Times</h3>
<ol>
<li><strong>Check GPU utilization</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>watch -n <span style="color:#ae81ff">1</span> <span style="color:#e6db74">&#39;kubectl exec -n ollama deployment/ollama-gpu -- nvidia-smi&#39;</span>
</span></span></code></pre></div><ol start="2">
<li>
<p><strong>Reduce parallel requests</strong>:</p>
<ul>
<li>Lower <code>OLLAMA_NUM_PARALLEL</code> value</li>
</ul>
</li>
<li>
<p><strong>Use smaller models</strong> for faster inference</p>
</li>
</ol>
<h3 id="model-download-issues">Model Download Issues</h3>
<ol>
<li><strong>Check model manager logs</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl logs -n ollama -l app<span style="color:#f92672">=</span>ollama-gpu -c model-manager
</span></span></code></pre></div><ol start="2">
<li><strong>Verify storage space</strong>:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl exec -n ollama deployment/ollama-gpu -- df -h /root/.ollama
</span></span></code></pre></div><h2 id="performance-tuning">Performance Tuning</h2>
<h3 id="gpu-optimization">GPU Optimization</h3>
<ul>
<li><strong>Layer allocation</strong>: <code>OLLAMA_GPU_LAYERS=999</code> ensures full GPU usage</li>
<li><strong>Memory management</strong>: Leave ~500MB for system overhead</li>
<li><strong>Batch processing</strong>: Adjust <code>OLLAMA_NUM_PARALLEL</code> based on workload</li>
</ul>
<h3 id="model-selection">Model Selection</h3>
<p>Choose models based on use case:</p>
<ul>
<li><strong>Coding</strong>: <code>deepseek-coder</code>, <code>deepseek-r1-coder-tools</code></li>
<li><strong>General chat</strong>: <code>llama3.1:8b</code>, <code>qwen3:8b</code></li>
<li><strong>Resource-constrained</strong>: <code>qwen2.5:0.5b</code>, <code>phi3:mini</code></li>
</ul>
<h3 id="memory-optimization">Memory Optimization</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># For limited GPU memory</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">OLLAMA_GPU_MEMORY</span>: <span style="color:#e6db74">&#34;11G&#34;</span>  <span style="color:#75715e"># Leave more for system</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">OLLAMA_MAX_LOADED_MODELS</span>: <span style="color:#e6db74">&#34;1&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">OLLAMA_KEEP_ALIVE</span>: <span style="color:#e6db74">&#34;1m&#34;</span>
</span></span></code></pre></div><h2 id="security-considerations">Security Considerations</h2>
<ul>
<li>Service runs with limited permissions</li>
<li>CORS is open (<code>OLLAMA_ORIGINS: &quot;*&quot;</code>) - restrict in production</li>
<li>No authentication by default - consider adding proxy authentication</li>
<li>Models are stored with fsGroup 1000 permissions</li>
</ul>
<h2 id="monitoring">Monitoring</h2>
<p>Key metrics to monitor:</p>
<ul>
<li>GPU utilization and memory usage</li>
<li>Model loading/unloading frequency</li>
<li>API response times</li>
<li>Error rates per model</li>
<li>Storage usage trends</li>
</ul>
<h3 id="prometheus-metrics">Prometheus Metrics</h3>
<p>Ollama exposes metrics at <code>/metrics</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://ollama-gpu.ollama.svc.cluster.local:11434/metrics
</span></span></code></pre></div><h2 id="backup-and-recovery">Backup and Recovery</h2>
<h3 id="model-backup">Model Backup</h3>
<p>Models are stored in the PVC. To backup:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Create backup job</span>
</span></span><span style="display:flex;"><span>kubectl create job ollama-backup -n ollama --from<span style="color:#f92672">=</span>cronjob/backup-ollama
</span></span></code></pre></div><h3 id="recovery">Recovery</h3>
<ol>
<li>Restore PVC from backup</li>
<li>Restart deployment to reload models</li>
</ol>
<h2 id="integration-examples">Integration Examples</h2>
<h3 id="python-client">Python Client</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> requests
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">query_ollama</span>(prompt, model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;llama3.1:8b&#34;</span>):
</span></span><span style="display:flex;"><span>    url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://ollama-gpu.ollama.svc.cluster.local:11434/api/generate&#34;</span>
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>post(url, json<span style="color:#f92672">=</span>{
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;model&#34;</span>: model,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;prompt&#34;</span>: prompt,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;stream&#34;</span>: <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    })
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> response<span style="color:#f92672">.</span>json()[<span style="color:#e6db74">&#39;response&#39;</span>]
</span></span></code></pre></div><h3 id="nodejs-integration">Node.js Integration</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">axios</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">require</span>(<span style="color:#e6db74">&#39;axios&#39;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">function</span> <span style="color:#a6e22e">generateText</span>(<span style="color:#a6e22e">prompt</span>) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">response</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> <span style="color:#a6e22e">axios</span>.<span style="color:#a6e22e">post</span>(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;http://ollama-gpu.ollama.svc.cluster.local:11434/api/generate&#39;</span>,
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#a6e22e">model</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#39;llama3.1:8b&#39;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#a6e22e">prompt</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">prompt</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#a6e22e">stream</span><span style="color:#f92672">:</span> <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    );
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">response</span>.<span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">response</span>;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="future-improvements">Future Improvements</h2>
<ul>
<li><input disabled="" type="checkbox"> Add authentication layer</li>
<li><input disabled="" type="checkbox"> Implement model version pinning</li>
<li><input disabled="" type="checkbox"> Create Grafana dashboards</li>
<li><input disabled="" type="checkbox"> Add automatic model pruning based on usage</li>
<li><input disabled="" type="checkbox"> Implement request queuing for better resource management</li>
<li><input disabled="" type="checkbox"> Add support for fine-tuned models</li>
<li><input disabled="" type="checkbox"> Create model recommendation system based on task</li>
</ul>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
</div>
    <small class="footer_copyright">
        © 2025 .
        
    </small>
</footer>







    
    <script async src="http://localhost:1313/js/main.js" ></script>

    

</body>
</html>
