<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pages on My New Hugo Site</title>
    <link>http://localhost:1313/pages/</link>
    <description>Recent content in Pages on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/pages/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fako Cluster Architecture Overview</title>
      <link>http://localhost:1313/pages/architecture/overview/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/architecture/overview/</guid>
      
      <description>&lt;h1 id=&#34;fako-cluster-architecture-overview&#34;&gt;Fako Cluster Architecture Overview&lt;/h1&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The Fako Cluster is a Kubernetes-based platform designed with modern cloud-native principles, emphasizing automation, security, and observability. This document provides a high-level overview of the cluster&amp;rsquo;s architecture and core components.&lt;/p&gt;
&lt;h2 id=&#34;core-architecture-principles&#34;&gt;Core Architecture Principles&lt;/h2&gt;
&lt;h3 id=&#34;gitops-driven-deployment&#34;&gt;GitOps-Driven Deployment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FluxCD&lt;/strong&gt; manages all deployments through Git repositories&lt;/li&gt;
&lt;li&gt;Infrastructure as Code (IaC) approach for all configurations&lt;/li&gt;
&lt;li&gt;Automated reconciliation ensures cluster state matches Git repository&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;security-first-design&#34;&gt;Security-First Design&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;External Secrets Operator&lt;/strong&gt; for secure secrets management&lt;/li&gt;
&lt;li&gt;Integration with AWS Secrets Manager&lt;/li&gt;
&lt;li&gt;SOPS encryption for sensitive configuration data&lt;/li&gt;
&lt;li&gt;Network policies and RBAC for defense in depth&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;high-availability&#34;&gt;High Availability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multi-node cluster configuration&lt;/li&gt;
&lt;li&gt;Persistent volume claims for stateful workloads&lt;/li&gt;
&lt;li&gt;Regular automated backups&lt;/li&gt;
&lt;li&gt;Health checks and auto-recovery mechanisms&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;infrastructure-stack&#34;&gt;Infrastructure Stack&lt;/h2&gt;
&lt;h3 id=&#34;container-orchestration&#34;&gt;Container Orchestration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;: Core orchestration platform&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;K3s&lt;/strong&gt;: Lightweight Kubernetes distribution for edge computing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;networking&#34;&gt;Networking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NGINX Ingress Controller&lt;/strong&gt;: HTTP/HTTPS routing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cert-Manager&lt;/strong&gt;: Automated TLS certificate management&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloudflare Tunnels&lt;/strong&gt;: Secure external access without exposed ports&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NFS&lt;/strong&gt;: Network-attached storage for persistent volumes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local Path Provisioner&lt;/strong&gt;: Dynamic volume provisioning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;monitoring--observability&#34;&gt;Monitoring &amp;amp; Observability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prometheus&lt;/strong&gt;: Metrics collection and alerting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grafana&lt;/strong&gt;: Visualization and dashboards&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loki&lt;/strong&gt;: Log aggregation and querying&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;application-architecture&#34;&gt;Application Architecture&lt;/h2&gt;
&lt;h3 id=&#34;service-categories&#34;&gt;Service Categories&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Core Services&lt;/strong&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Ollama AI Model Server</title>
      <link>http://localhost:1313/pages/projects/ollama/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/projects/ollama/</guid>
      
      <description>&lt;h1 id=&#34;ollama-ai-model-server&#34;&gt;Ollama AI Model Server&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Ollama is a local AI model server deployed in the Fako cluster that provides a unified API for running large language models (LLMs). It runs on GPU acceleration and manages multiple AI models, making them available for various applications including Home Assistant integrations, coding assistants, and general AI tasks.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU Acceleration&lt;/strong&gt;: Optimized for RTX 5070 with 12GB VRAM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Management&lt;/strong&gt;: Automatic downloading and lifecycle management of models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiple Model Support&lt;/strong&gt;: Hosts various models from small (0.5B) to large (32B+)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RESTful API&lt;/strong&gt;: Compatible with OpenAI-style APIs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Persistent Storage&lt;/strong&gt;: Models cached to avoid re-downloading&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto-scaling&lt;/strong&gt;: Memory management with model unloading&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica GPU deployment pinned to the &lt;code&gt;yeezyai&lt;/code&gt; node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Services&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;ClusterIP service on port 11434&lt;/li&gt;
&lt;li&gt;NodePort service for external access&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PersistentVolumeClaim for model storage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMaps&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;General configuration&lt;/li&gt;
&lt;li&gt;GPU-specific settings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sidecar Container&lt;/strong&gt;: Model manager for automatic model downloads&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: 1 NVIDIA GPU (RTX 5070)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 12Gi (request), 36Gi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 4 cores (request), 8 cores (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Persistent volume for model storage&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;general-settings-ollama-configmap&#34;&gt;General Settings (ollama-configmap)&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Default&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_HOST&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Bind address for the API server&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_ORIGINS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;*&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;CORS origins allowed&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_MODELS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;/root/.ollama/models&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Model storage directory&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_DEBUG&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Enable debug logging&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_NOHISTORY&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Keep conversation history&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;gpu-settings-ollama-gpu-configmap&#34;&gt;GPU Settings (ollama-gpu-configmap)&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Default&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_GPU_LAYERS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;999&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Force all layers to GPU&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_GPU_MEMORY&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;12G&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;GPU memory allocation&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_NUM_PARALLEL&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;2&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Concurrent request handling&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_NUM_THREAD&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;8&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;CPU threads for operations&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_MAX_LOADED_MODELS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Models kept in GPU memory&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;OLLAMA_KEEP_ALIVE&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;5m&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Model keep-alive duration&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;available-models&#34;&gt;Available Models&lt;/h2&gt;
&lt;p&gt;The model manager automatically downloads and maintains these models:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Open WebUI - AI Chat Interface</title>
      <link>http://localhost:1313/pages/projects/open-webui/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/projects/open-webui/</guid>
      
      <description>&lt;h1 id=&#34;open-webui---ai-chat-interface&#34;&gt;Open WebUI - AI Chat Interface&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Open WebUI is a feature-rich, user-friendly web interface for AI language models deployed in the Fako cluster. It provides a ChatGPT-like experience with support for multiple AI backends. This deployment is configured to use GPUStack as the primary AI backend, providing access to various open-source language models through an OpenAI-compatible API.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Modern Chat Interface&lt;/strong&gt;: Clean, responsive UI similar to ChatGPT&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: Access multiple AI models from GPUStack&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Management&lt;/strong&gt;: Built-in authentication and user roles&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conversation History&lt;/strong&gt;: Persistent chat storage with SQLite&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Switching&lt;/strong&gt;: Easy switching between available models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom System Prompts&lt;/strong&gt;: Create and save custom prompts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dark/Light Mode&lt;/strong&gt;: Theme switching for user preference&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Responsive&lt;/strong&gt;: Works seamlessly on all devices&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica stateful deployment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: ClusterIP service on port 8080&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ingress&lt;/strong&gt;: HTTPS access for external users&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PersistentVolumeClaim for SQLite database and user data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMap&lt;/strong&gt;: Application configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;External Secrets&lt;/strong&gt;: API keys and endpoints from AWS Secrets Manager&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 3Gi (request), 6Gi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 1.5 cores (request), 3 cores (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Persistent volume for database and uploads&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;application-settings&#34;&gt;Application Settings&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Value&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;ENABLE_OLLAMA_API&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Ollama disabled, using GPUStack&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;ENABLE_OPENAI_API&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;OpenAI-compatible API enabled&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;WEBUI_NAME&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;HomeLab AI (GPUStack) - Production&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Instance name&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;WEBUI_AUTH&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Authentication enabled&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;ENABLE_SIGNUP&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;New signups disabled&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;DEFAULT_MODELS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;gemma-3-27b-it,glm4-0414&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Available models&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;BYPASS_MODEL_ACCESS_CONTROL&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;All models visible to all users&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;backend-configuration&#34;&gt;Backend Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AI Backend&lt;/strong&gt;: GPUStack (OpenAI-compatible)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: SQLite at &lt;code&gt;/app/backend/data/webui.db&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Session Lifetime&lt;/strong&gt;: 30 days (2592000 seconds)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Request Timeout&lt;/strong&gt;: 10 minutes for large models&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;h3 id=&#34;accessing-open-webui&#34;&gt;Accessing Open WebUI&lt;/h3&gt;
&lt;p&gt;External access:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>N8N Workflow Automation</title>
      <link>http://localhost:1313/pages/projects/n8n/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/projects/n8n/</guid>
      
      <description>&lt;h1 id=&#34;n8n-workflow-automation-service&#34;&gt;N8N Workflow Automation Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;N8N is an open-source workflow automation tool deployed in the Fako cluster. It provides a visual interface for creating complex automation workflows, integrating with hundreds of services and APIs. N8N allows you to automate repetitive tasks, connect different services, and build sophisticated data pipelines without extensive coding.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Visual Workflow Builder&lt;/strong&gt;: Drag-and-drop interface for creating automations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;200+ Integrations&lt;/strong&gt;: Pre-built nodes for popular services&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self-Hosted&lt;/strong&gt;: Full control over your data and workflows&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PostgreSQL Backend&lt;/strong&gt;: Reliable storage using the cluster&amp;rsquo;s PostgreSQL&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Webhook Support&lt;/strong&gt;: Trigger workflows via HTTP webhooks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Custom Functions&lt;/strong&gt;: JavaScript code support for complex logic&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fair-Code License&lt;/strong&gt;: Source available with sustainable business model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica deployment (stateful workflows)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: ClusterIP service on port 5678&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ingress&lt;/strong&gt;: HTTPS access at &lt;code&gt;n8n.landryzetam.net&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PersistentVolumeClaim for workflow data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: PostgreSQL for workflow definitions and execution data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;External Secrets&lt;/strong&gt;: Database credentials from AWS Secrets Manager&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 512Mi (request), 2Gi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 250m (request), 1000m (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Persistent volume for N8N data and files&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: Uses shared PostgreSQL cluster&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;environment-settings&#34;&gt;Environment Settings&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Value&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;N8N_HOST&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;n8n.landryzetam.net&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;External hostname&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;N8N_PORT&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;5678&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Application port&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;N8N_PROTOCOL&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;https&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;External protocol&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;WEBHOOK_URL&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;https://n8n.landryzetam.net/&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Webhook base URL&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;GENERIC_TIMEZONE&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;America/New_York&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Timezone for scheduling&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;N8N_METRICS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Enable metrics endpoint&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;N8N_TEMPLATES_ENABLED&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Enable workflow templates&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;database-configuration&#34;&gt;Database Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: PostgreSQL&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Host&lt;/strong&gt;: &lt;code&gt;postgres-cluster-rw.postgres.svc.cluster.local&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: &lt;code&gt;app&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User&lt;/strong&gt;: &lt;code&gt;app&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Password&lt;/strong&gt;: Managed via External Secrets&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;h3 id=&#34;accessing-n8n&#34;&gt;Accessing N8N&lt;/h3&gt;
&lt;p&gt;External access:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Oura Dashboard - Health Data Visualization</title>
      <link>http://localhost:1313/pages/projects/oura-dashboard/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/projects/oura-dashboard/</guid>
      
      <description>&lt;h1 id=&#34;oura-dashboard---health-data-visualization-platform&#34;&gt;Oura Dashboard - Health Data Visualization Platform&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Oura Dashboard is a Streamlit-based web application deployed in the Fako cluster that provides interactive visualization of health data collected from the Oura Ring. It connects to the PostgreSQL database populated by the Oura Collector service and presents comprehensive health insights through charts, graphs, and analytics. The dashboard is secured with OAuth2 authentication and provides personalized health tracking capabilities for sleep, activity, readiness, and heart rate metrics.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>About</title>
      <link>http://localhost:1313/pages/about/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/about/</guid>
      
      <description>&lt;h1 id=&#34;landry-and-sheer-curiosity&#34;&gt;Landry and Sheer Curiosity&lt;/h1&gt;
&lt;p&gt;Welcome to my corner of the internet where technology meets curiosity. This site serves as a documentation hub for the Fako Cluster - a personal Kubernetes infrastructure project that showcases modern cloud-native technologies and best practices.&lt;/p&gt;
&lt;h2 id=&#34;what-youll-find-here&#34;&gt;What You&amp;rsquo;ll Find Here&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Architecture Guides&lt;/strong&gt;: Deep dives into the cluster&amp;rsquo;s design and implementation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service Documentation&lt;/strong&gt;: Detailed information about each deployed service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Project Showcases&lt;/strong&gt;: Various technology projects and experiments&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;: Real-world applications and integrations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-fako-cluster&#34;&gt;The Fako Cluster&lt;/h2&gt;
&lt;p&gt;The Fako Cluster is a self-hosted Kubernetes environment that demonstrates:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Projects</title>
      <link>http://localhost:1313/pages/projects/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/projects/</guid>
      
      <description>&lt;h1 id=&#34;projects&#34;&gt;Projects&lt;/h1&gt;
&lt;p&gt;Welcome to the projects section! Here you&amp;rsquo;ll find detailed documentation about the various services and applications running on the Fako Cluster. Each project represents a real-world implementation of modern cloud-native technologies.&lt;/p&gt;
&lt;h2 id=&#34;featured-projects&#34;&gt;Featured Projects&lt;/h2&gt;
&lt;h3 id=&#34;ai--machine-learning&#34;&gt;AI &amp;amp; Machine Learning&lt;/h3&gt;
&lt;h4 id=&#34;ollama-ai-model-server&#34;&gt;&lt;a href=&#34;http://localhost:1313/pages/projects/ollama&#34;&gt;Ollama AI Model Server&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Run large language models locally with GPU acceleration. This project showcases how to self-host AI models for privacy, cost savings, and unlimited usage.&lt;/p&gt;
&lt;h4 id=&#34;open-webui---ai-chat-interface&#34;&gt;&lt;a href=&#34;http://localhost:1313/pages/projects/open-webui&#34;&gt;Open WebUI - AI Chat Interface&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;A ChatGPT-like web interface for interacting with local AI models. Features user management, conversation history, and seamless integration with GPUStack.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/guides/documenting-your-setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/guides/documenting-your-setup/</guid>
      
      <description>&lt;h1 id=&#34;how-to-document-your-fako-cluster-setup&#34;&gt;How to Document Your Fako Cluster Setup&lt;/h1&gt;
&lt;p&gt;This guide will help you systematically document all aspects of your Fako cluster using the Hugo blog system.&lt;/p&gt;
&lt;h2 id=&#34;overview-of-the-documentation-system&#34;&gt;Overview of the Documentation System&lt;/h2&gt;
&lt;p&gt;Your documentation system consists of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Central docs directory&lt;/strong&gt;: &lt;code&gt;/docs/&lt;/code&gt; in your repository&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hugo blog&lt;/strong&gt;: Automatically syncs and serves documentation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CronJob&lt;/strong&gt;: Updates the blog every 6 hours from GitHub&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;step-by-step-documentation-process&#34;&gt;Step-by-Step Documentation Process&lt;/h2&gt;
&lt;h3 id=&#34;1-document-your-services&#34;&gt;1. Document Your Services&lt;/h3&gt;
&lt;p&gt;For each service in &lt;code&gt;apps/base/&lt;/code&gt;, create documentation using the template:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/infrastructure/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/infrastructure/readme/</guid>
      
      <description>&lt;h1 id=&#34;fako-cluster-documentation&#34;&gt;Fako Cluster Documentation&lt;/h1&gt;
&lt;p&gt;Welcome to the comprehensive documentation for the Fako Cluster. This documentation covers all services, infrastructure components, and operational procedures.&lt;/p&gt;
&lt;h2 id=&#34;documentation-structure&#34;&gt;Documentation Structure&lt;/h2&gt;
&lt;h3 id=&#34;-services&#34;&gt;ðŸ“¦ &lt;a href=&#34;./services/&#34;&gt;Services&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Detailed documentation for each service deployed in the cluster:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AI/ML Services&lt;/strong&gt;: Whisper, Piper, Ollama, GPUStack&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monitoring &amp;amp; Security&lt;/strong&gt;: Kubescape, Kube-bench, Gitleaks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Management Tools&lt;/strong&gt;: Headlamp, Kagent, Keycloak&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Application Services&lt;/strong&gt;: N8N, Blog, Linkding, Audiobookshelf&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MCP Servers&lt;/strong&gt;: Various Model Context Protocol servers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: PostgreSQL cluster, pgAdmin&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Health &amp;amp; Fitness&lt;/strong&gt;: Wger, Oura collector/dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-active-use-cases&#34;&gt;ðŸš€ &lt;a href=&#34;./use-cases/&#34;&gt;Active Use Cases&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Real-world implementations and practical applications:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/operations/cluster-maintenance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/operations/cluster-maintenance/</guid>
      
      <description>&lt;h1 id=&#34;cluster-maintenance-operations&#34;&gt;Cluster Maintenance Operations&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This document outlines standard maintenance procedures for the Fako Cluster, including routine tasks, troubleshooting steps, and emergency procedures.&lt;/p&gt;
&lt;h2 id=&#34;daily-operations&#34;&gt;Daily Operations&lt;/h2&gt;
&lt;h3 id=&#34;health-checks&#34;&gt;Health Checks&lt;/h3&gt;
&lt;h4 id=&#34;check-cluster-status&#34;&gt;Check Cluster Status&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get nodes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get pods --all-namespaces | grep -v Running
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl top nodes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl top pods --all-namespaces
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;check-fluxcd-sync-status&#34;&gt;Check FluxCD Sync Status&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;flux get all
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;flux logs --all-namespaces
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;log-review&#34;&gt;Log Review&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Check for any failed pods or containers&lt;/li&gt;
&lt;li&gt;Review ingress logs for unusual traffic patterns&lt;/li&gt;
&lt;li&gt;Monitor resource usage trends&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;weekly-maintenance&#34;&gt;Weekly Maintenance&lt;/h2&gt;
&lt;h3 id=&#34;backup-verification&#34;&gt;Backup Verification&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Verify all PVC backups are current&lt;/li&gt;
&lt;li&gt;Test restore procedures for critical services&lt;/li&gt;
&lt;li&gt;Document any backup failures&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;certificate-status&#34;&gt;Certificate Status&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get certificates --all-namespaces
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl describe certificate -n cert-manager
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;security-scans&#34;&gt;Security Scans&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Run Gitleaks scan&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl create job --from&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;cronjob/gitleaks-scan -n gitleaks manual-scan-&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;date +%s&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Check Kubescape results&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl logs -n kubescape-operator -l app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kubescape
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;monthly-tasks&#34;&gt;Monthly Tasks&lt;/h2&gt;
&lt;h3 id=&#34;updates-and-patches&#34;&gt;Updates and Patches&lt;/h3&gt;
&lt;h4 id=&#34;update-flux-components&#34;&gt;Update Flux Components&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;flux check --pre
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;flux install --export &amp;gt; flux-system/gotk-components.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git add flux-system/gotk-components.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git commit -m &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Update Flux components&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git push
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;update-container-images&#34;&gt;Update Container Images&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Review container image versions&lt;/li&gt;
&lt;li&gt;Test updates in staging environment&lt;/li&gt;
&lt;li&gt;Apply updates through Git commits&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resource-optimization&#34;&gt;Resource Optimization&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Check resource requests vs actual usage&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl top pods --all-namespaces --sort-by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;cpu
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl top pods --all-namespaces --sort-by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;memory
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;troubleshooting-procedures&#34;&gt;Troubleshooting Procedures&lt;/h2&gt;
&lt;h3 id=&#34;pod-issues&#34;&gt;Pod Issues&lt;/h3&gt;
&lt;h4 id=&#34;pod-stuck-in-pending&#34;&gt;Pod Stuck in Pending&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl describe pod &amp;lt;pod-name&amp;gt; -n &amp;lt;namespace&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get events -n &amp;lt;namespace&amp;gt; --sort-by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.lastTimestamp&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;pod-crashloopbackoff&#34;&gt;Pod CrashLoopBackOff&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl logs &amp;lt;pod-name&amp;gt; -n &amp;lt;namespace&amp;gt; --previous
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl describe pod &amp;lt;pod-name&amp;gt; -n &amp;lt;namespace&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;storage-issues&#34;&gt;Storage Issues&lt;/h3&gt;
&lt;h4 id=&#34;pvc-not-binding&#34;&gt;PVC Not Binding&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get pv
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get pvc -n &amp;lt;namespace&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl describe pvc &amp;lt;pvc-name&amp;gt; -n &amp;lt;namespace&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;nfs-mount-issues&#34;&gt;NFS Mount Issues&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Check NFS server connectivity&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;showmount -e &amp;lt;nfs-server-ip&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Check mount inside pod&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl exec -it &amp;lt;pod-name&amp;gt; -n &amp;lt;namespace&amp;gt; -- df -h
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;networking-issues&#34;&gt;Networking Issues&lt;/h3&gt;
&lt;h4 id=&#34;service-not-accessible&#34;&gt;Service Not Accessible&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get svc -n &amp;lt;namespace&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get endpoints -n &amp;lt;namespace&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl describe ingress -n &amp;lt;namespace&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;dns-resolution&#34;&gt;DNS Resolution&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl run -it --rm debug --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;busybox --restart&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Never -- nslookup &amp;lt;service-name&amp;gt;.&amp;lt;namespace&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;emergency-procedures&#34;&gt;Emergency Procedures&lt;/h2&gt;
&lt;h3 id=&#34;cluster-recovery&#34;&gt;Cluster Recovery&lt;/h3&gt;
&lt;h4 id=&#34;node-failure&#34;&gt;Node Failure&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Cordon the node: &lt;code&gt;kubectl cordon &amp;lt;node-name&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Drain workloads: &lt;code&gt;kubectl drain &amp;lt;node-name&amp;gt; --ignore-daemonsets&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fix node issues&lt;/li&gt;
&lt;li&gt;Uncordon node: &lt;code&gt;kubectl uncordon &amp;lt;node-name&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;etcd-backup-and-restore&#34;&gt;Etcd Backup and Restore&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Backup etcd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ETCDCTL_API&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; etcdctl snapshot save backup.db &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --endpoints&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;https://127.0.0.1:2379 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --cacert&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/etc/kubernetes/pki/etcd/ca.crt &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --cert&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/etc/kubernetes/pki/etcd/healthcheck-client.crt &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/etc/kubernetes/pki/etcd/healthcheck-client.key
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Verify backup&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ETCDCTL_API&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; etcdctl --write-out&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;table snapshot status backup.db
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;disaster-recovery&#34;&gt;Disaster Recovery&lt;/h3&gt;
&lt;h4 id=&#34;full-cluster-restore&#34;&gt;Full Cluster Restore&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Restore etcd from backup&lt;/li&gt;
&lt;li&gt;Re-apply Flux bootstrap&lt;/li&gt;
&lt;li&gt;Wait for GitOps reconciliation&lt;/li&gt;
&lt;li&gt;Verify all services are running&lt;/li&gt;
&lt;li&gt;Restore persistent data from backups&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;monitoring-and-alerts&#34;&gt;Monitoring and Alerts&lt;/h2&gt;
&lt;h3 id=&#34;key-metrics-to-monitor&#34;&gt;Key Metrics to Monitor&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CPU and Memory usage per node&lt;/li&gt;
&lt;li&gt;Disk usage on persistent volumes&lt;/li&gt;
&lt;li&gt;Network ingress/egress rates&lt;/li&gt;
&lt;li&gt;Pod restart counts&lt;/li&gt;
&lt;li&gt;Certificate expiration dates&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;alert-response&#34;&gt;Alert Response&lt;/h3&gt;
&lt;h4 id=&#34;high-resource-usage&#34;&gt;High Resource Usage&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Identify resource-consuming pods&lt;/li&gt;
&lt;li&gt;Check for memory leaks or runaway processes&lt;/li&gt;
&lt;li&gt;Scale horizontally if needed&lt;/li&gt;
&lt;li&gt;Consider resource limit adjustments&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;service-degradation&#34;&gt;Service Degradation&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Check pod health and logs&lt;/li&gt;
&lt;li&gt;Verify external dependencies&lt;/li&gt;
&lt;li&gt;Review recent changes via Git history&lt;/li&gt;
&lt;li&gt;Rollback if necessary using Flux&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;documentation-updates&#34;&gt;Documentation Updates&lt;/h2&gt;
&lt;h3 id=&#34;after-each-incident&#34;&gt;After Each Incident&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Document root cause&lt;/li&gt;
&lt;li&gt;Update runbooks&lt;/li&gt;
&lt;li&gt;Create or update alerts&lt;/li&gt;
&lt;li&gt;Share lessons learned&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;regular-reviews&#34;&gt;Regular Reviews&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Monthly review of procedures&lt;/li&gt;
&lt;li&gt;Quarterly disaster recovery drills&lt;/li&gt;
&lt;li&gt;Annual security audit&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/audiobookshelf/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/audiobookshelf/readme/</guid>
      
      <description>&lt;h1 id=&#34;audiobookshelf---audiobook-and-podcast-server&#34;&gt;Audiobookshelf - Audiobook and Podcast Server&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Audiobookshelf is a self-hosted audiobook and podcast server deployed in the Fako cluster. It provides a comprehensive media management system with features like automatic metadata fetching, progress syncing across devices, and a beautiful web interface. Audiobookshelf supports various audio formats and offers mobile apps for iOS and Android, making it a complete solution for managing and enjoying your audio library.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multi-Format Support&lt;/strong&gt;: MP3, M4A, M4B, FLAC, OGG, and more&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Automatic Metadata&lt;/strong&gt;: Fetches cover art, descriptions, and metadata&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Progress Sync&lt;/strong&gt;: Syncs playback position across all devices&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Apps&lt;/strong&gt;: Native iOS and Android applications&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-User&lt;/strong&gt;: Support for multiple users with individual progress&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Series Management&lt;/strong&gt;: Organize books by series and authors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Podcast Support&lt;/strong&gt;: Subscribe and manage podcast feeds&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sleep Timer&lt;/strong&gt;: Built-in sleep timer for bedtime listening&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Variable Playback Speed&lt;/strong&gt;: Adjust playback speed to preference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica deployment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: ClusterIP service on port 3005&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Three PersistentVolumeClaims:
&lt;ul&gt;
&lt;li&gt;Config: Application configuration and database&lt;/li&gt;
&lt;li&gt;Metadata: Cover images and cached metadata&lt;/li&gt;
&lt;li&gt;Audiobooks: Media library storage&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMap&lt;/strong&gt;: Basic configuration settings&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security Context&lt;/strong&gt;: Runs as user 1000&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 512Mi (request), 1Gi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 250m (request), 1000m (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Config volume for database&lt;/li&gt;
&lt;li&gt;Metadata volume for images/cache&lt;/li&gt;
&lt;li&gt;Audiobooks volume for media files&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;deployment-settings&#34;&gt;Deployment Settings&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Value&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Image&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;ghcr.io/advplyr/audiobookshelf:2.26.0&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Audiobookshelf version&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Port&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;3005&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Web interface port&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;User/Group&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;1000&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Non-root user&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Volumes&lt;/td&gt;
          &lt;td&gt;3 mounts&lt;/td&gt;
          &lt;td&gt;Config, metadata, and media&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;volume-mounts&#34;&gt;Volume Mounts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/config&lt;/code&gt;: Database and configuration files&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/metadata&lt;/code&gt;: Cover art and metadata cache&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/audiobooks&lt;/code&gt;: Media library root directory&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;h3 id=&#34;accessing-audiobookshelf&#34;&gt;Accessing Audiobookshelf&lt;/h3&gt;
&lt;p&gt;Internal access:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/blog/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/blog/readme/</guid>
      
      <description>&lt;h1 id=&#34;hugo-blog-documentation-service&#34;&gt;Hugo Blog Documentation Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The Hugo Blog service is a static site generator that serves as the central documentation hub for the Fako cluster. It automatically collects and organizes documentation from across the repository, presenting it in a searchable, user-friendly website. The service uses Hugo with automatic synchronization from GitHub.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Automatic Documentation Collection&lt;/strong&gt;: CronJob syncs docs every 6 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Static Site Generation&lt;/strong&gt;: Fast, secure Hugo-based site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GitHub Integration&lt;/strong&gt;: Pulls documentation directly from repository&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Organized Structure&lt;/strong&gt;: Categorized documentation (services, guides, architecture)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Minimal Deployment&lt;/strong&gt;: Simple Hugo server with basic theme&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Persistent Storage&lt;/strong&gt;: Documentation cached in PVC&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica Hugo server deployment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: ClusterIP service on port 80&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ingress&lt;/strong&gt;: HTTPS access at &lt;code&gt;blog.fako-cluster.local&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: 5Gi PersistentVolumeClaim for content&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMaps&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Documentation copy scripts&lt;/li&gt;
&lt;li&gt;Hugo configuration (if customized)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CronJob&lt;/strong&gt;: Automatic GitHub synchronization&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 128Mi (request), 256Mi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 100m (request), 500m (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: 5Gi for Hugo site content&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;hugo-server-configuration&#34;&gt;Hugo Server Configuration&lt;/h3&gt;
&lt;p&gt;The deployment creates a minimal Hugo site on startup with:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/gitleaks/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/gitleaks/readme/</guid>
      
      <description>&lt;h1 id=&#34;gitleaks---secret-detection-and-cleanup-service&#34;&gt;Gitleaks - Secret Detection and Cleanup Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Gitleaks is an automated secret scanning and cleanup service deployed in the Fako cluster. It runs as a scheduled CronJob that scans Git repositories for exposed secrets, API keys, and sensitive information. When secrets are detected, it can automatically remove them from the entire Git history using BFG Repo-Cleaner. This service helps maintain security by preventing accidental exposure of credentials in version control.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/gpustack-proxy/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/gpustack-proxy/readme/</guid>
      
      <description>&lt;h1 id=&#34;gpustack-proxy---secure-tunnel-access-service&#34;&gt;GPUStack Proxy - Secure Tunnel Access Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;GPUStack Proxy is a Cloudflare Tunnel-based proxy service deployed in the Fako cluster that provides secure external access to the GPUStack AI inference platform. It uses Cloudflare&amp;rsquo;s cloudflared to create an encrypted tunnel between the cluster and Cloudflare&amp;rsquo;s edge network, eliminating the need for exposed public IPs or open inbound ports. The service includes automatic endpoint synchronization with AWS Secrets Manager for seamless integration with AI applications.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/headlamp/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/headlamp/readme/</guid>
      
      <description>&lt;h1 id=&#34;headlamp-kubernetes-dashboard&#34;&gt;Headlamp Kubernetes Dashboard&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Headlamp is a user-friendly Kubernetes dashboard deployed in the Fako cluster. It provides a web-based UI for managing and monitoring Kubernetes resources across all namespaces. Headlamp offers an intuitive interface for both beginners and advanced users, with support for plugins and real-time updates. This deployment includes the Kubescape security plugin for enhanced cluster security visibility.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User-Friendly Interface&lt;/strong&gt;: Clean, modern UI for Kubernetes management&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-Time Updates&lt;/strong&gt;: Live view of cluster resources and events&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Resource Support&lt;/strong&gt;: Manage all Kubernetes resource types&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin Architecture&lt;/strong&gt;: Extensible with custom plugins&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In-Cluster Mode&lt;/strong&gt;: Runs with cluster service account&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubescape Integration&lt;/strong&gt;: Security scanning and compliance checks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Responsive&lt;/strong&gt;: Works on desktop and mobile devices&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica deployment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: ClusterIP service on port 80&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ingress&lt;/strong&gt;: HTTPS access (configured separately)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ServiceAccount&lt;/strong&gt;: Cluster-wide permissions for resource access&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Init Container&lt;/strong&gt;: Installs Kubescape plugin&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin Volume&lt;/strong&gt;: EmptyDir for plugin storage&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 128Mi (request), 512Mi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 100m (request), 500m (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: EmptyDir for plugins (ephemeral)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;deployment-settings&#34;&gt;Deployment Settings&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Value&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;-in-cluster&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Enabled&lt;/td&gt;
          &lt;td&gt;Run using in-cluster config&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;-plugins-dir&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;/build/plugins&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Plugin directory location&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Port&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;4466&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Application HTTP port&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Image&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;ghcr.io/headlamp-k8s/headlamp:v0.33.0&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Headlamp version&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;rbac-configuration&#34;&gt;RBAC Configuration&lt;/h3&gt;
&lt;p&gt;Headlamp runs with a ClusterRole that provides full access to:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/housekeeping/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/housekeeping/readme/</guid>
      
      <description>&lt;h1 id=&#34;housekeeping---cluster-cleanup-service&#34;&gt;Housekeeping - Cluster Cleanup Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Housekeeping is an automated cluster maintenance service deployed in the Fako cluster. It runs as a CronJob that periodically cleans up completed and failed pods across all namespaces. This service helps maintain cluster hygiene by removing terminated pods that would otherwise accumulate and consume resources. It&amp;rsquo;s essential for keeping the cluster clean and preventing resource exhaustion from leftover pod metadata.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Automated Cleanup&lt;/strong&gt;: Runs every hour to clean terminated pods&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cluster-Wide&lt;/strong&gt;: Scans all namespaces for cleanup candidates&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Safe Deletion&lt;/strong&gt;: Only removes pods in Succeeded or Failed states&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Detailed Logging&lt;/strong&gt;: Reports what was found and cleaned&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concurrency Control&lt;/strong&gt;: Prevents overlapping cleanup runs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistics Tracking&lt;/strong&gt;: Counts total pods found and deleted&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Force Deletion&lt;/strong&gt;: Uses grace period 0 for immediate removal&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Job History&lt;/strong&gt;: Keeps history of last 3 successful/failed runs&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;CronJob&lt;/strong&gt;: Scheduled execution every hour&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ServiceAccount&lt;/strong&gt;: Cluster-wide permissions for pod management&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ClusterRole&lt;/strong&gt;: Permissions to list and delete pods&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMap&lt;/strong&gt;: Cleanup script with logic&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container&lt;/strong&gt;: kubectl image with bash script execution&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image&lt;/strong&gt;: &lt;code&gt;bitnami/kubectl:1.33&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU/Memory&lt;/strong&gt;: Minimal (kubectl operations)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Permissions&lt;/strong&gt;: Cluster-wide pod list/delete&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;schedule&#34;&gt;Schedule&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CronJob Schedule&lt;/strong&gt;: &lt;code&gt;0 * * * *&lt;/code&gt; (every hour at minute 0)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concurrency Policy&lt;/strong&gt;: &lt;code&gt;Forbid&lt;/code&gt; (no overlapping runs)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TTL After Finish&lt;/strong&gt;: 3600 seconds (1 hour)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;History Limits&lt;/strong&gt;: 3 successful, 3 failed jobs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cleanup-targets&#34;&gt;Cleanup Targets&lt;/h3&gt;
&lt;p&gt;The service removes pods in these states:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/kagent/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/kagent/readme/</guid>
      
      <description>&lt;h1 id=&#34;kagent---kubernetes-ai-agent-framework&#34;&gt;KAgent - Kubernetes AI Agent Framework&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;KAgent is a Kubernetes-native AI agent framework deployed in the Fako cluster. It provides a platform for deploying and managing AI agents that can interact with various systems through the Model Context Protocol (MCP). Agents can be configured with different AI models, specialized knowledge, and tool access. The framework supports multiple AI providers including OpenAI, Ollama, and GPUStack, making it flexible for various use cases from Kubernetes operations to code generation.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/keycloak/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/keycloak/readme/</guid>
      
      <description>&lt;h1 id=&#34;keycloak-identity-and-access-management&#34;&gt;Keycloak Identity and Access Management&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Keycloak is an open-source Identity and Access Management (IAM) solution deployed in the Fako cluster. It provides single sign-on (SSO), user federation, identity brokering, and social login capabilities. This deployment uses Keycloak 26.x with PostgreSQL backend and is configured for high availability with clustering.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single Sign-On (SSO)&lt;/strong&gt;: One login for multiple applications&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identity Brokering&lt;/strong&gt;: Connect to external identity providers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Federation&lt;/strong&gt;: Sync users from LDAP or Active Directory&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Social Login&lt;/strong&gt;: Support for Google, GitHub, Facebook, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Factor Authentication&lt;/strong&gt;: Enhanced security with MFA&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High Availability&lt;/strong&gt;: Clustered deployment with 2 replicas&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Secure Credential Management&lt;/strong&gt;: Integration with AWS Secrets Manager&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: 2-replica deployment with pod anti-affinity&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Services&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Main service on port 8080&lt;/li&gt;
&lt;li&gt;Headless service for JGroups clustering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PostgreSQL database (postgres-cluster)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMap&lt;/strong&gt;: Environment configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;External Secrets&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Admin credentials from AWS Secrets Manager&lt;/li&gt;
&lt;li&gt;Database credentials from AWS Secrets Manager&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Init Container&lt;/strong&gt;: Database initialization and user setup&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 1Gi (request), 2Gi (limit) per pod&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 500m (request), 1000m (limit) per pod&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: PostgreSQL with dedicated database and user&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;environment-settings&#34;&gt;Environment Settings&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Value&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;KC_HOSTNAME&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;https://auth.landryzetam.net&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;External hostname&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;KC_HOSTNAME_STRICT&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Allow backend access&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;KC_HTTP_ENABLED&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Enable HTTP (behind proxy)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;KC_PROXY&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;edge&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Running behind edge proxy&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;KC_PROXY_HEADERS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;xforwarded&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Use X-Forwarded headers&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;KC_HEALTH_ENABLED&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Enable health endpoints&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;KC_METRICS_ENABLED&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Enable metrics&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;KC_CACHE_STACK&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;kubernetes&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Use Kubernetes discovery&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;database-configuration&#34;&gt;Database Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: PostgreSQL&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Host&lt;/strong&gt;: &lt;code&gt;postgres-cluster-rw.postgres.svc.cluster.local&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Port&lt;/strong&gt;: 5432&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: &lt;code&gt;keycloak&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Credentials&lt;/strong&gt;: Managed via AWS Secrets Manager&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;clustering-configuration&#34;&gt;Clustering Configuration&lt;/h3&gt;
&lt;p&gt;JGroups clustering is configured for session replication:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/kube-bench/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/kube-bench/readme/</guid>
      
      <description>&lt;h1 id=&#34;kube-bench---kubernetes-security-benchmark-scanner&#34;&gt;Kube-bench - Kubernetes Security Benchmark Scanner&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Kube-bench is an automated security compliance scanner deployed in the Fako cluster. It runs the CIS Kubernetes Benchmark checks to ensure the cluster follows security best practices. The service executes as a daily CronJob that performs comprehensive security scans, checking configurations of various Kubernetes components against industry standards. Results are saved with timestamps for audit trails and compliance reporting.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CIS Benchmark Compliance&lt;/strong&gt;: Checks against Center for Internet Security standards&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Automated Scanning&lt;/strong&gt;: Daily scheduled security assessments&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comprehensive Coverage&lt;/strong&gt;: Scans master and worker node configurations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JSON Output&lt;/strong&gt;: Machine-readable results for integration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Historical Records&lt;/strong&gt;: Timestamped results for trend analysis&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Host Access&lt;/strong&gt;: Direct access to Kubernetes configuration files&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Component&lt;/strong&gt;: Checks etcd, kubelet, API server, and more&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Results Processing&lt;/strong&gt;: Automatic display of scan findings&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;CronJob&lt;/strong&gt;: Daily execution at 2 AM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ServiceAccount&lt;/strong&gt;: Permissions for scanning operations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Main Container&lt;/strong&gt;: kube-bench scanner with host access&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Results Processor&lt;/strong&gt;: Monitors and displays scan results&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PersistentVolumeClaim for scan history&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Host Mounts&lt;/strong&gt;: Access to Kubernetes system directories&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scanner&lt;/strong&gt;: 256Mi RAM (request), 512Mi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Processor&lt;/strong&gt;: 64Mi RAM (request), 128Mi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 100m-300m for scanner, 50m-100m for processor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Persistent volume for results history&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;schedule&#34;&gt;Schedule&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CronJob Schedule&lt;/strong&gt;: &lt;code&gt;0 2 * * *&lt;/code&gt; (daily at 2:00 AM)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concurrency Policy&lt;/strong&gt;: &lt;code&gt;Forbid&lt;/code&gt; (no overlapping scans)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;History Limits&lt;/strong&gt;: 3 successful, 3 failed jobs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;security-context&#34;&gt;Security Context&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;hostPID&lt;/strong&gt;: &lt;code&gt;true&lt;/code&gt; (access host processes)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;privileged&lt;/strong&gt;: &lt;code&gt;true&lt;/code&gt; (full host access)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read-only mounts&lt;/strong&gt;: All host paths mounted read-only&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;h3 id=&#34;manual-scan-execution&#34;&gt;Manual Scan Execution&lt;/h3&gt;
&lt;p&gt;Trigger an immediate security scan:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/kubescape-operator/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/kubescape-operator/readme/</guid>
      
      <description>&lt;h1 id=&#34;kubescape-operator---kubernetes-security-scanning-platform&#34;&gt;Kubescape Operator - Kubernetes Security Scanning Platform&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Kubescape Operator is a comprehensive Kubernetes security scanning platform deployed in the Fako cluster. It provides continuous security assessment using multiple frameworks including NSA, MITRE, and CIS benchmarks. The operator performs real-time vulnerability scanning, compliance checking, and network policy analysis. It stores scan results locally and can optionally integrate with Kubescape Cloud for enhanced dashboards and reporting.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multi-Framework Scanning&lt;/strong&gt;: NSA, MITRE, CIS compliance checks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Continuous Monitoring&lt;/strong&gt;: Real-time security posture assessment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vulnerability Scanning&lt;/strong&gt;: Container image vulnerability detection&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network Policy Analysis&lt;/strong&gt;: Automated network policy recommendations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local Storage&lt;/strong&gt;: Persistent storage for scan results and vulnerability DB&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prometheus Metrics&lt;/strong&gt;: Export security metrics for monitoring&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Node Agent&lt;/strong&gt;: Runtime security monitoring on Linux nodes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scheduled Scans&lt;/strong&gt;: Configurable scanning intervals&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Operator&lt;/strong&gt;: Core controller managing scans&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kubevuln&lt;/strong&gt;: Vulnerability scanner component&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gateway&lt;/strong&gt;: API gateway for results&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Node Agent&lt;/strong&gt;: Runtime security on each node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: NFS-backed persistent volumes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Helm Release&lt;/strong&gt;: Managed by Flux&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Component&lt;/th&gt;
          &lt;th&gt;Memory Request&lt;/th&gt;
          &lt;th&gt;Memory Limit&lt;/th&gt;
          &lt;th&gt;CPU Request&lt;/th&gt;
          &lt;th&gt;CPU Limit&lt;/th&gt;
          &lt;th&gt;Ephemeral Storage&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Operator&lt;/td&gt;
          &lt;td&gt;256Mi&lt;/td&gt;
          &lt;td&gt;512Mi&lt;/td&gt;
          &lt;td&gt;100m&lt;/td&gt;
          &lt;td&gt;500m&lt;/td&gt;
          &lt;td&gt;1Gi/2Gi&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Kubevuln&lt;/td&gt;
          &lt;td&gt;512Mi&lt;/td&gt;
          &lt;td&gt;1Gi&lt;/td&gt;
          &lt;td&gt;200m&lt;/td&gt;
          &lt;td&gt;500m&lt;/td&gt;
          &lt;td&gt;5Gi/15Gi&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Gateway&lt;/td&gt;
          &lt;td&gt;256Mi&lt;/td&gt;
          &lt;td&gt;512Mi&lt;/td&gt;
          &lt;td&gt;100m&lt;/td&gt;
          &lt;td&gt;300m&lt;/td&gt;
          &lt;td&gt;1Gi/2Gi&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;scanning-schedule&#34;&gt;Scanning Schedule&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Framework&lt;/th&gt;
          &lt;th&gt;Schedule&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;NSA&lt;/td&gt;
          &lt;td&gt;Every 6 hours&lt;/td&gt;
          &lt;td&gt;National Security Agency guidelines&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;MITRE&lt;/td&gt;
          &lt;td&gt;Every 6 hours&lt;/td&gt;
          &lt;td&gt;MITRE ATT&amp;amp;CK framework&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;CIS v1.23&lt;/td&gt;
          &lt;td&gt;Daily at 2 AM&lt;/td&gt;
          &lt;td&gt;Center for Internet Security benchmark&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;storage-configuration&#34;&gt;Storage Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scan Results&lt;/strong&gt;: 20Gi on &lt;code&gt;nfs-security-logs&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vulnerability DB&lt;/strong&gt;: 10Gi on &lt;code&gt;nfs-security-logs&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Access Mode&lt;/strong&gt;: ReadWriteOnce&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;h3 id=&#34;viewing-scan-results&#34;&gt;Viewing Scan Results&lt;/h3&gt;
&lt;p&gt;Access scan results via kubectl:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/linkding/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/linkding/readme/</guid>
      
      <description>&lt;h1 id=&#34;linkding---bookmark-management-service&#34;&gt;Linkding - Bookmark Management Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Linkding is a self-hosted bookmark manager deployed in the Fako cluster. It provides a clean, minimal web interface for saving, organizing, and searching bookmarks. Linkding offers features like automatic title and description retrieval, tagging, full-text search, and bookmark archiving. It&amp;rsquo;s designed as a privacy-focused alternative to commercial bookmark services, keeping all your data under your control.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Privacy-Focused&lt;/strong&gt;: Self-hosted with no external dependencies&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clean Interface&lt;/strong&gt;: Minimalist, fast web UI&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tagging System&lt;/strong&gt;: Organize bookmarks with tags&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Full-Text Search&lt;/strong&gt;: Search titles, descriptions, and tags&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Auto-Completion&lt;/strong&gt;: Title and description fetching&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Archive Support&lt;/strong&gt;: Archive bookmarks with Wayback Machine&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Import/Export&lt;/strong&gt;: Supports standard bookmark formats&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;REST API&lt;/strong&gt;: Programmatic access to bookmarks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile-Friendly&lt;/strong&gt;: Responsive design for all devices&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica deployment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: ClusterIP service on port 9090&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PersistentVolumeClaim for bookmark data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Secret&lt;/strong&gt;: Environment variables for configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security Context&lt;/strong&gt;: Runs as www-data user (UID 33)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 128Mi (request), 256Mi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 250m (request), 500m (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Persistent volume for database and assets&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;deployment-settings&#34;&gt;Deployment Settings&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Value&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Image&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;sissbruecker/linkding:1.41.0&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Linkding version&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Port&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;9090&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Web interface port&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;User&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;www-data (33)&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Run as non-root user&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Data Path&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;/etc/linkding/data&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Persistent data location&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;environment-variables&#34;&gt;Environment Variables&lt;/h3&gt;
&lt;p&gt;Configuration via &lt;code&gt;linkding-container-env&lt;/code&gt; secret:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/mcp-servers/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/mcp-servers/readme/</guid>
      
      <description>&lt;h1 id=&#34;mcp-servers---model-context-protocol-services&#34;&gt;MCP Servers - Model Context Protocol Services&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;MCP (Model Context Protocol) Servers is a collection of specialized services deployed in the Fako cluster that extend AI capabilities by providing structured access to various tools and resources. These servers implement the Model Context Protocol, enabling AI assistants like Claude to interact with external systems, databases, APIs, and tools in a standardized way. Each MCP server provides specific functionality that can be consumed by AI agents for enhanced automation and integration.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/node-labeling/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/node-labeling/readme/</guid>
      
      <description>&lt;h1 id=&#34;node-labeling---kubernetes-node-classification-service&#34;&gt;Node Labeling - Kubernetes Node Classification Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Node Labeling is an automated Kubernetes job deployed in the Fako cluster that applies strategic labels to cluster nodes. It classifies nodes based on their hardware capabilities, particularly focusing on GPU resources and performance tiers. This service ensures that workloads can be properly scheduled to appropriate nodes using node selectors and affinity rules. The labeling is crucial for GPU workloads, voice pipeline services, and general workload distribution across the heterogeneous cluster.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/ollama-webui/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/ollama-webui/readme/</guid>
      
      <description>&lt;h1 id=&#34;ollama-webui---test-instance-of-open-webui&#34;&gt;Ollama WebUI - Test Instance of Open WebUI&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Ollama WebUI is a test/staging deployment of Open WebUI in the Fako cluster. Despite its name, this service actually uses the Open WebUI image and is configured to connect to GPUStack (not Ollama) as its AI backend. It serves as a testing environment for new configurations, model testing, and user onboarding before changes are promoted to the production Open WebUI instance. This separation allows for safe experimentation without affecting production users.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/openwakeword/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/openwakeword/readme/</guid>
      
      <description>&lt;h1 id=&#34;openwakeword---wake-word-detection-service&#34;&gt;OpenWakeWord - Wake Word Detection Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;OpenWakeWord is a wake word detection service deployed in the Fako cluster as part of the voice assistant pipeline. It uses machine learning models to detect specific wake words (like &amp;ldquo;Hey Jarvis&amp;rdquo;) in continuous audio streams. The service implements the Wyoming protocol for integration with voice assistants and can handle multiple concurrent audio streams. It&amp;rsquo;s designed for low-latency, real-time wake word detection and scales automatically based on load.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/oura-collector/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/oura-collector/readme/</guid>
      
      <description>&lt;h1 id=&#34;oura-collector---health-data-collection-service&#34;&gt;Oura Collector - Health Data Collection Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Oura Collector is an automated health data collection service deployed in the Fako cluster. It integrates with the Oura Ring API to collect personal health metrics including sleep, activity, readiness, and heart rate data. The service stores this data in PostgreSQL for analysis and visualization. It features scheduled collection, backfill capabilities, and secure credential management through AWS Secrets Manager. This enables long-term health tracking and analysis within your personal infrastructure.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/pgadmin/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/pgadmin/readme/</guid>
      
      <description>&lt;h1 id=&#34;pgadmin-postgresql-management-tool&#34;&gt;pgAdmin PostgreSQL Management Tool&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;pgAdmin is a comprehensive web-based management tool for PostgreSQL deployed in the Fako cluster. It provides a graphical interface for database administration, query execution, and monitoring. This deployment is pre-configured to connect to the PostgreSQL cluster with both read-write and read-only endpoints, making database management accessible through a web browser.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Web-Based Interface&lt;/strong&gt;: Full-featured PostgreSQL management from browser&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pre-Configured Servers&lt;/strong&gt;: Automatic connection to cluster databases&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Query Tool&lt;/strong&gt;: Advanced SQL editor with syntax highlighting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visual Database Designer&lt;/strong&gt;: ERD and schema visualization&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backup/Restore&lt;/strong&gt;: GUI for database backup operations&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Server Monitoring&lt;/strong&gt;: Real-time database statistics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;User Management&lt;/strong&gt;: Graphical user and role administration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica deployment&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: ClusterIP service on port 80&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ingress&lt;/strong&gt;: HTTPS access (when configured)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PersistentVolumeClaim for pgAdmin data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMap&lt;/strong&gt;: Server configuration and connection settings&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;External Secrets&lt;/strong&gt;: Database passwords from AWS Secrets Manager&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 256Mi (request), 512Mi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 250m (request), 500m (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Persistent volume for pgAdmin data and sessions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;environment-settings&#34;&gt;Environment Settings&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Value&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;PGADMIN_DEFAULT_EMAIL&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;85landry@gmail.com&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Admin login email&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;PGADMIN_CONFIG_SERVER_MODE&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Desktop mode (single user)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;No master password needed&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;PGADMIN_SERVER_JSON_FILE&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;/pgadmin4/servers.json&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Pre-configured servers&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;pre-configured-database-servers&#34;&gt;Pre-Configured Database Servers&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PostgreSQL Cluster (Read-Write)&lt;/strong&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/piper/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/piper/readme/</guid>
      
      <description>&lt;h1 id=&#34;piper-text-to-speech-service&#34;&gt;Piper Text-to-Speech Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Piper is a fast, local text-to-speech (TTS) service deployed in the Fako cluster. It converts text into natural-sounding speech using neural voice models and exposes its functionality via the Wyoming protocol, making it compatible with voice assistant platforms like Home Assistant.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CPU-Optimized&lt;/strong&gt;: Runs efficiently on CPU without requiring GPU&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wyoming Protocol&lt;/strong&gt;: Compatible with Home Assistant and other voice platforms&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiple Voice Support&lt;/strong&gt;: Various voice models with different qualities&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Voice Management&lt;/strong&gt;: Automatic voice model management via sidecar container&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Persistent Storage&lt;/strong&gt;: Voice models cached to avoid re-downloading&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Supports multiple concurrent TTS processes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica deployment with node affinity rules&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: ClusterIP service on port 10200 (Wyoming protocol)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PersistentVolumeClaim for voice model storage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMap&lt;/strong&gt;: Configuration for default voice settings&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sidecar Container&lt;/strong&gt;: Voice manager for monitoring voice models&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: Not required (CPU-only)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 4Gi (request), 8Gi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 2 cores (request), 4 cores (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Persistent volume for voice model storage&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;environment-settings&#34;&gt;Environment Settings&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Default&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;DEFAULT_VOICE&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;en_US-amy-medium&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Default voice model to use&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;--max-piper-procs&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;8&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Maximum concurrent TTS processes&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;--data-dir&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;/data&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Directory for voice data&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;--download-dir&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;/data&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Directory for downloading voices&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;available-voice-models&#34;&gt;Available Voice Models&lt;/h3&gt;
&lt;p&gt;Piper supports various voice models with different qualities:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/postgres-cluster/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/postgres-cluster/readme/</guid>
      
      <description>&lt;h1 id=&#34;postgresql-cluster-service&#34;&gt;PostgreSQL Cluster Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The PostgreSQL Cluster is a highly available database service deployed using CloudNative PG (CNPG) operator. It provides a 3-node PostgreSQL 16.2 cluster with automatic failover, backup scheduling, and secure credential management through AWS Secrets Manager. This cluster serves as the primary database backend for multiple services including Keycloak, N8N, and other applications.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High Availability&lt;/strong&gt;: 3-node cluster with automatic failover&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Automated Backups&lt;/strong&gt;: Daily scheduled backups at 2 AM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Secure Credentials&lt;/strong&gt;: Integration with AWS Secrets Manager&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Connection Pooling&lt;/strong&gt;: PgBouncer pooler for efficient connections&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Tuned&lt;/strong&gt;: Optimized PostgreSQL parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pod Anti-Affinity&lt;/strong&gt;: Instances spread across different nodes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Monitoring Ready&lt;/strong&gt;: Prometheus metrics support&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Cluster&lt;/strong&gt;: 3 PostgreSQL instances (1 primary, 2 replicas)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Operator&lt;/strong&gt;: CloudNative PG operator manages the cluster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: 50Gi persistent volume per instance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pooler&lt;/strong&gt;: PgBouncer connection pooler&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backup&lt;/strong&gt;: Scheduled backup with retention policy&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;External Secrets&lt;/strong&gt;: AWS Secrets Manager integration&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 2Gi (request), 4Gi (limit) per instance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 1 core (request), 2 cores (limit) per instance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: 50Gi per instance (150Gi total)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage Class&lt;/strong&gt;: &lt;code&gt;nfs-postgres-v2&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;postgresql-parameters&#34;&gt;PostgreSQL Parameters&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Value&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;max_connections&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;200&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Maximum concurrent connections&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;shared_buffers&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;512MB&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Memory for caching data&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;effective_cache_size&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;3GB&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Planner&amp;rsquo;s assumption about cache&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;work_mem&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;8MB&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Memory for query operations&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;wal_buffers&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;16MB&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;WAL write buffer size&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;maintenance_work_mem&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;64MB&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Memory for maintenance operations&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;max_wal_size&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;1GB&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Maximum WAL size before checkpoint&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;checkpoint_timeout&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;5min&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Time between checkpoints&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;network-access&#34;&gt;Network Access&lt;/h3&gt;
&lt;p&gt;The cluster allows connections from:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/service_template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/service_template/</guid>
      
      <description>&lt;h1 id=&#34;service-name&#34;&gt;[Service Name]&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;[Brief description of what the service does and its purpose in the cluster]&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Feature 1&lt;/strong&gt;: [Description]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feature 2&lt;/strong&gt;: [Description]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Feature 3&lt;/strong&gt;: [Description]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: [Description of deployment strategy]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: [Type of service and ports]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: [Storage requirements if any]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMap/Secrets&lt;/strong&gt;: [Configuration management]&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: [If applicable]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: [Request/Limit]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: [Request/Limit]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: [PVC size if applicable]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;environment-variables&#34;&gt;Environment Variables&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Variable&lt;/th&gt;
          &lt;th&gt;Default&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;VAR_1&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;value&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Description&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;VAR_2&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;value&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Description&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;configuration-files&#34;&gt;Configuration Files&lt;/h3&gt;
&lt;p&gt;[Describe any configuration files and their purpose]&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/wger/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/wger/readme/</guid>
      
      <description>&lt;h1 id=&#34;wger---fitness-and-workout-manager&#34;&gt;wger - Fitness and Workout Manager&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;wger (Workout Manager) is a free, open-source fitness and workout tracking application deployed in the Fako cluster. It provides comprehensive features for managing workouts, tracking nutrition, monitoring body weight, and creating exercise routines. The application includes a large exercise database with images and videos, making it an excellent self-hosted alternative to commercial fitness apps. It&amp;rsquo;s accessible via the web interface and offers both registered user accounts and guest access.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/services/whisper/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/services/whisper/readme/</guid>
      
      <description>&lt;h1 id=&#34;whisper-speech-to-text-service&#34;&gt;Whisper Speech-to-Text Service&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Whisper is an automatic speech recognition (ASR) service deployed in the Fako cluster. It uses OpenAI&amp;rsquo;s Whisper model running on GPU acceleration to provide high-quality speech-to-text transcription. The service is exposed via the Wyoming protocol, making it compatible with various voice assistant platforms.&lt;/p&gt;
&lt;h2 id=&#34;key-features&#34;&gt;Key Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU Acceleration&lt;/strong&gt;: Runs on NVIDIA GPU for fast transcription&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wyoming Protocol&lt;/strong&gt;: Compatible with Home Assistant and other voice platforms&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multiple Model Sizes&lt;/strong&gt;: Configurable model size (base, small, medium, large)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Voice Activity Detection&lt;/strong&gt;: Built-in VAD for efficient processing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Persistent Model Storage&lt;/strong&gt;: Models are cached to avoid re-downloading&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Single-replica GPU deployment pinned to the &lt;code&gt;yeezyai&lt;/code&gt; node&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: ClusterIP service on port 10300 (Wyoming protocol)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: PersistentVolumeClaim for model caching&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ConfigMap&lt;/strong&gt;: Environment configuration for model parameters&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;resource-requirements&#34;&gt;Resource Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: 1 NVIDIA GPU (required)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 2Gi (request), 4Gi (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 2 cores (request), 4 cores (limit)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Persistent volume for model storage&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configuration&#34;&gt;Configuration&lt;/h2&gt;
&lt;h3 id=&#34;model-parameters-via-configmap&#34;&gt;Model Parameters (via ConfigMap)&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Default&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;MODEL&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;base&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Whisper model size (tiny, base, small, medium, large)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;LANGUAGE&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;en&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Primary language for transcription&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;COMPUTE_TYPE&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;float16&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;GPU compute precision&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;BEAM_SIZE&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Beam search width for decoding&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;voice-activity-detection&#34;&gt;Voice Activity Detection&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Parameter&lt;/th&gt;
          &lt;th&gt;Default&lt;/th&gt;
          &lt;th&gt;Description&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;VAD_THRESHOLD&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;0.5&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Voice activity detection threshold&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;MIN_SPEECH_DURATION_MS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;250&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Minimum speech duration in milliseconds&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;MAX_SPEECH_DURATION_S&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Maximum speech duration in seconds&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;code&gt;SPEECH_PAD_MS&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;&lt;code&gt;400&lt;/code&gt;&lt;/td&gt;
          &lt;td&gt;Padding around detected speech&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;h3 id=&#34;accessing-the-service&#34;&gt;Accessing the Service&lt;/h3&gt;
&lt;p&gt;The Whisper service is available within the cluster at:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/pages/use-cases/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/pages/use-cases/readme/</guid>
      
      <description>&lt;h1 id=&#34;active-use-cases---fako-cluster-in-action&#34;&gt;Active Use Cases - Fako Cluster in Action&lt;/h1&gt;
&lt;p&gt;This section documents real-world use cases and practical applications of the Fako cluster services. Each use case includes detailed walkthroughs, screenshots, and implementation details.&lt;/p&gt;
&lt;h2 id=&#34;-table-of-contents&#34;&gt;ðŸ“‹ Table of Contents&lt;/h2&gt;
&lt;h3 id=&#34;aiml-use-cases&#34;&gt;AI/ML Use Cases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#personal-ai-assistant&#34;&gt;Personal AI Assistant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#voice-controlled-home-automation&#34;&gt;Voice-Controlled Home Automation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#document-analysis-pipeline&#34;&gt;Document Analysis Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multi-model-ai-comparison&#34;&gt;Multi-Model AI Comparison&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;development--automation&#34;&gt;Development &amp;amp; Automation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gitops-workflow-management&#34;&gt;GitOps Workflow Management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#automated-security-scanning&#34;&gt;Automated Security Scanning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cicd-pipeline-integration&#34;&gt;CI/CD Pipeline Integration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;personal-productivity&#34;&gt;Personal Productivity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#health-data-analytics&#34;&gt;Health Data Analytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#knowledge-management-system&#34;&gt;Knowledge Management System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#media-server-setup&#34;&gt;Media Server Setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;infrastructure-management&#34;&gt;Infrastructure Management&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#multi-node-gpu-scheduling&#34;&gt;Multi-Node GPU Scheduling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#secure-remote-access&#34;&gt;Secure Remote Access&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#automated-backup-strategy&#34;&gt;Automated Backup Strategy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-personal-ai-assistant&#34;&gt;ðŸ¤– Personal AI Assistant&lt;/h2&gt;
&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;A comprehensive AI assistant setup using Open WebUI, Ollama, and GPUStack for various AI models.&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
