<!DOCTYPE html>
<html lang="en-us"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">Running Local AI: My Experience with Ollama and GPU Acceleration | My New Hugo Site</title>
<meta property="og:title" content="Running Local AI: My Experience with Ollama and GPU Acceleration | My New Hugo Site" />
<meta name="twitter:title" content="Running Local AI: My Experience with Ollama and GPU Acceleration | My New Hugo Site" />
<meta itemprop="name" content="Running Local AI: My Experience with Ollama and GPU Acceleration | My New Hugo Site" />
<meta name="application-name" content="Running Local AI: My Experience with Ollama and GPU Acceleration | My New Hugo Site" />
<meta property="og:site_name" content="" />

<meta name="description" content="Practical insights from running local LLMs with Ollama on Kubernetes using GPU acceleration">
<meta itemprop="description" content="Practical insights from running local LLMs with Ollama on Kubernetes using GPU acceleration" />
<meta property="og:description" content="Practical insights from running local LLMs with Ollama on Kubernetes using GPU acceleration" />
<meta name="twitter:description" content="Practical insights from running local LLMs with Ollama on Kubernetes using GPU acceleration" />

<meta property="og:locale" content="en-us" />
<meta name="language" content="en-us" />

  <link rel="alternate" hreflang="en" href="http://localhost:1313/posts/local-ai-with-ollama/" title="" />





    
    
    

    <meta property="og:type" content="article" />
    <meta property="og:article:published_time" content=2025-01-07T00:00:00Z />
    <meta property="article:published_time" content=2025-01-07T00:00:00Z />
    <meta property="og:url" content="http://localhost:1313/posts/local-ai-with-ollama/" />

    

    

    <script defer type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "Article",
        "headline": "Running Local AI: My Experience with Ollama and GPU Acceleration",
        "author": {
        "@type": "Person",
        "name": ""
        },
        "datePublished": "2025-01-07",
        "description": "Practical insights from running local LLMs with Ollama on Kubernetes using GPU acceleration",
        "wordCount":  796 ,
        "mainEntityOfPage": "True",
        "dateModified": "2025-01-07",
        "image": {
        "@type": "imageObject",
        "url": ""
        },
        "publisher": {
        "@type": "Organization",
        "name": "My New Hugo Site"
        }
    }
    </script>


<meta name="generator" content="Hugo 0.148.2">

    
    <meta property="og:url" content="http://localhost:1313/posts/local-ai-with-ollama/">
  <meta property="og:site_name" content="My New Hugo Site">
  <meta property="og:title" content="Running Local AI: My Experience with Ollama and GPU Acceleration">
  <meta property="og:description" content="Practical insights from running local LLMs with Ollama on Kubernetes using GPU acceleration">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-01-07T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-07T00:00:00+00:00">
    <meta property="article:tag" content="AI">
    <meta property="article:tag" content="Ollama">
    <meta property="article:tag" content="GPU">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Self-Hosting">
    <meta property="article:tag" content="Machine Learning">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Running Local AI: My Experience with Ollama and GPU Acceleration">
  <meta name="twitter:description" content="Practical insights from running local LLMs with Ollama on Kubernetes using GPU acceleration">


    

    <link rel="canonical" href="http://localhost:1313/posts/local-ai-with-ollama/">
    <link href="/style.min.e390ba7da26222f4dc42a349955d76dbbe44e5ce2535a43de5a70633a0a9ec3c.css" rel="stylesheet">
    <link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/favicon.ico">




<link rel="manifest" href="http://localhost:1313/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/icons/favicon.svg">

    
    
</head>
<body data-theme = "" class="notransition">

<script src="/js/theme.js"></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="http://localhost:1313/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title></title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="/pages/about/">
                        About
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/pages/projects/">
                        Projects
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/secondbrain/">
                        Second Brain
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">Running Local AI: My Experience with Ollama and GPU Acceleration</h1>
                
                
                
                <div class="post-meta">
                    <time datetime="2025-01-07T00:00:00&#43;00:00" itemprop="datePublished"> Jan 7, 2025 </time>
                </div>
                
            </header>
            
            <div class="page-content">
                <p>One of the most exciting aspects of the Fako Cluster is the ability to run large language models locally. With privacy concerns around cloud AI services and the desire for unlimited API usage, I decided to dive deep into self-hosting LLMs using Ollama. Here&rsquo;s what I&rsquo;ve learned.</p>
<h2 id="the-hardware-foundation">The Hardware Foundation</h2>
<p>Running LLMs locally requires serious GPU power. My setup:</p>
<ul>
<li><strong>GPU</strong>: NVIDIA RTX 5070 with 12GB VRAM</li>
<li><strong>Node</strong>: Dedicated <code>yeezyai</code> node in the cluster</li>
<li><strong>Memory</strong>: 36GB RAM allocated to the Ollama pod</li>
</ul>
<p>This configuration allows me to run models up to 32B parameters, though performance varies significantly based on model size.</p>
<h2 id="why-ollama">Why Ollama?</h2>
<p>After evaluating several options (LocalAI, llama.cpp, text-generation-webui), I chose Ollama because:</p>
<ol>
<li><strong>Simple API</strong>: OpenAI-compatible endpoints</li>
<li><strong>Model Management</strong>: Automatic downloading and caching</li>
<li><strong>Resource Efficiency</strong>: Smart memory management</li>
<li><strong>Kubernetes-Ready</strong>: Easy to containerize and scale</li>
</ol>
<h2 id="real-world-performance">Real-World Performance</h2>
<p>Here&rsquo;s what I&rsquo;ve observed running different model sizes:</p>
<h3 id="small-models-05b---3b-parameters">Small Models (0.5B - 3B parameters)</h3>
<ul>
<li><strong>Examples</strong>: qwen2.5:0.5b, llama3.2:3b</li>
<li><strong>Performance</strong>: Nearly instant responses</li>
<li><strong>Use Cases</strong>: Quick tasks, code completion, chat</li>
<li><strong>Memory</strong>: 1-3GB VRAM</li>
</ul>
<h3 id="medium-models-6b---8b-parameters">Medium Models (6B - 8B parameters)</h3>
<ul>
<li><strong>Examples</strong>: llama3.1:8b, deepseek-coder:6.7b</li>
<li><strong>Performance</strong>: 20-40 tokens/second</li>
<li><strong>Use Cases</strong>: General purpose, coding assistance</li>
<li><strong>Memory</strong>: 4-8GB VRAM</li>
</ul>
<h3 id="large-models-13b---32b-parameters">Large Models (13B - 32B parameters)</h3>
<ul>
<li><strong>Examples</strong>: deepseek-r1:14b, qwen:32b</li>
<li><strong>Performance</strong>: 5-15 tokens/second</li>
<li><strong>Use Cases</strong>: Complex reasoning, detailed analysis</li>
<li><strong>Memory</strong>: 8-20GB VRAM (with quantization)</li>
</ul>
<h2 id="practical-implementation">Practical Implementation</h2>
<h3 id="the-deployment-strategy">The Deployment Strategy</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ollama-gpu</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">ollama</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">ollama-gpu</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">nodeSelector</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">kubernetes.io/hostname</span>: <span style="color:#ae81ff">yeezyai</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ollama</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">ollama/ollama:latest</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">nvidia.com/gpu</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">36Gi</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">OLLAMA_GPU_LAYERS</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;999&#34;</span>  <span style="color:#75715e"># Force all layers to GPU</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">OLLAMA_MAX_LOADED_MODELS</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;1&#34;</span>    <span style="color:#75715e"># Memory optimization</span>
</span></span></code></pre></div><h3 id="model-management-automation">Model Management Automation</h3>
<p>I created a sidecar container to automatically download and maintain models:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e">#!/bin/bash
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#75715e"># Model manager script</span>
</span></span><span style="display:flex;"><span>MODELS<span style="color:#f92672">=(</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;llama3.1:8b&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;deepseek-coder:6.7b-instruct&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;qwen3:8b&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> model in <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>MODELS[@]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>; <span style="color:#66d9ef">do</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> ! ollama list | grep -q <span style="color:#e6db74">&#34;</span>$model<span style="color:#e6db74">&#34;</span>; <span style="color:#66d9ef">then</span>
</span></span><span style="display:flex;"><span>    echo <span style="color:#e6db74">&#34;Pulling </span>$model<span style="color:#e6db74">...&#34;</span>
</span></span><span style="display:flex;"><span>    ollama pull <span style="color:#e6db74">&#34;</span>$model<span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">fi</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">done</span>
</span></span></code></pre></div><h2 id="integration-success-stories">Integration Success Stories</h2>
<h3 id="1-vs-code--cline">1. VS Code + Cline</h3>
<p>My favorite integration is with Cline (formerly Continue) in VS Code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">&#34;models&#34;</span>: [{
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;title&#34;</span>: <span style="color:#e6db74">&#34;DeepSeek Coder&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;provider&#34;</span>: <span style="color:#e6db74">&#34;ollama&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;model&#34;</span>: <span style="color:#e6db74">&#34;deepseek-coder:6.7b-instruct&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;apiBase&#34;</span>: <span style="color:#e6db74">&#34;http://localhost:11434&#34;</span>
</span></span><span style="display:flex;"><span>  }]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Result: Code completion and assistance without sending code to external services.</p>
<h3 id="2-home-assistant-voice-assistant">2. Home Assistant Voice Assistant</h3>
<p>Integrated with Home Assistant for voice commands:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">conversation</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">intents</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">HassLightControl</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">sentences</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#e6db74">&#34;turn on the {area} lights&#34;</span>
</span></span><span style="display:flex;"><span>          - <span style="color:#e6db74">&#34;lights on in {area}&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">openai_conversation</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;Local Ollama&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">api_key</span>: <span style="color:#e6db74">&#34;dummy&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">base_url</span>: <span style="color:#e6db74">&#34;http://ollama-gpu.ollama.svc.cluster.local:11434/v1&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">model</span>: <span style="color:#e6db74">&#34;qwen3:8b&#34;</span>
</span></span></code></pre></div><h3 id="3-n8n-workflow-automation">3. n8n Workflow Automation</h3>
<p>Using Ollama in n8n workflows for text processing:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span><span style="color:#75715e">// n8n Function node
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">response</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> <span style="color:#a6e22e">$http</span>.<span style="color:#a6e22e">post</span>(
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#39;http://ollama-gpu:11434/api/generate&#39;</span>,
</span></span><span style="display:flex;"><span>  {
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">model</span><span style="color:#f92672">:</span> <span style="color:#e6db74">&#39;llama3.1:8b&#39;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">prompt</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">items</span>[<span style="color:#ae81ff">0</span>].<span style="color:#a6e22e">json</span>.<span style="color:#a6e22e">text</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">stream</span><span style="color:#f92672">:</span> <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">return</span> [{<span style="color:#a6e22e">json</span><span style="color:#f92672">:</span> {<span style="color:#a6e22e">result</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">response</span>.<span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">response</span>}}];
</span></span></code></pre></div><h2 id="challenges-and-solutions">Challenges and Solutions</h2>
<h3 id="gpu-memory-management">GPU Memory Management</h3>
<p><strong>Problem</strong>: Models consuming all VRAM and crashing</p>
<p><strong>Solution</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">OLLAMA_GPU_MEMORY</span>: <span style="color:#e6db74">&#34;11G&#34;</span>  <span style="color:#75715e"># Leave 1GB for system</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">OLLAMA_KEEP_ALIVE</span>: <span style="color:#e6db74">&#34;5m&#34;</span>   <span style="color:#75715e"># Unload models after 5 minutes</span>
</span></span></code></pre></div><h3 id="model-selection-paralysis">Model Selection Paralysis</h3>
<p><strong>Problem</strong>: Too many models to choose from</p>
<p><strong>Solution</strong>: Created a decision matrix:</p>
<ul>
<li><strong>Coding</strong>: deepseek-coder or deepseek-r1</li>
<li><strong>General Chat</strong>: llama3.1:8b</li>
<li><strong>Quick Tasks</strong>: qwen2.5:0.5b</li>
<li><strong>Home Assistant</strong>: qwen3:8b (optimized for commands)</li>
</ul>
<h3 id="network-latency">Network Latency</h3>
<p><strong>Problem</strong>: Slow responses over network</p>
<p><strong>Solution</strong>:</p>
<ul>
<li>NodePort service for local access</li>
<li>Connection pooling in applications</li>
<li>Response streaming for better UX</li>
</ul>
<h2 id="cost-analysis">Cost Analysis</h2>
<p>Running Ollama locally vs cloud APIs:</p>
<table>
  <thead>
      <tr>
          <th>Service</th>
          <th>Monthly Cost</th>
          <th>Requests/Month</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>OpenAI GPT-4</td>
          <td>$300+</td>
          <td>100K</td>
      </tr>
      <tr>
          <td>Claude Pro</td>
          <td>$20</td>
          <td>Limited</td>
      </tr>
      <tr>
          <td><strong>Local Ollama</strong></td>
          <td>$0*</td>
          <td>Unlimited</td>
      </tr>
  </tbody>
</table>
<p>*Excluding electricity costs (~$10/month for 24/7 GPU usage)</p>
<h2 id="performance-optimization-tips">Performance Optimization Tips</h2>
<ol>
<li>
<p><strong>Quantization is Your Friend</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Use quantized models for better performance</span>
</span></span><span style="display:flex;"><span>ollama pull llama3.1:8b-instruct-q4_K_M
</span></span></code></pre></div></li>
<li>
<p><strong>Layer Allocation</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">OLLAMA_GPU_LAYERS</span>: <span style="color:#e6db74">&#34;999&#34;</span>  <span style="color:#75715e"># All layers on GPU</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">OLLAMA_NUM_THREAD</span>: <span style="color:#e6db74">&#34;8&#34;</span>    <span style="color:#75715e"># Match CPU cores</span>
</span></span></code></pre></div></li>
<li>
<p><strong>Batch Processing</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Process multiple prompts efficiently</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">async</span> <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch_inference</span>(prompts):
</span></span><span style="display:flex;"><span>    tasks <span style="color:#f92672">=</span> [ollama_generate(p) <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> prompts]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">await</span> asyncio<span style="color:#f92672">.</span>gather(<span style="color:#f92672">*</span>tasks)
</span></span></code></pre></div></li>
</ol>
<h2 id="future-plans">Future Plans</h2>
<p>My roadmap for local AI:</p>
<ol>
<li><strong>Fine-tuning Pipeline</strong>: Custom models for specific tasks</li>
<li><strong>Multi-GPU Support</strong>: Scale to larger models</li>
<li><strong>RAG Implementation</strong>: Local knowledge base integration</li>
<li><strong>Voice Pipeline</strong>: Real-time voice processing</li>
<li><strong>Model Mixing</strong>: Ensemble approaches for better results</li>
</ol>
<h2 id="lessons-learned">Lessons Learned</h2>
<ol>
<li><strong>Start Small</strong>: Begin with 7B models and scale up</li>
<li><strong>Monitor Everything</strong>: GPU metrics are crucial</li>
<li><strong>Cache Aggressively</strong>: Model loading is expensive</li>
<li><strong>Choose Wisely</strong>: Not every task needs a 32B model</li>
<li><strong>Embrace Quantization</strong>: Quality loss is minimal</li>
</ol>
<h2 id="is-it-worth-it">Is It Worth It?</h2>
<p>Absolutely! The benefits:</p>
<ul>
<li><strong>Privacy</strong>: Your data never leaves your infrastructure</li>
<li><strong>Cost</strong>: One-time hardware investment vs recurring API costs</li>
<li><strong>Control</strong>: Choose models, update on your schedule</li>
<li><strong>Learning</strong>: Deep understanding of LLM operations</li>
<li><strong>Integration</strong>: Seamless with local services</li>
</ul>
<h2 id="getting-started">Getting Started</h2>
<p>If you&rsquo;re interested in running local AI:</p>
<ol>
<li>Start with a decent GPU (8GB+ VRAM)</li>
<li>Use Ollama for simplicity</li>
<li>Begin with smaller models</li>
<li>Monitor performance closely</li>
<li>Scale based on actual needs</li>
</ol>
<p>The journey from cloud-dependent AI to fully local inference has been enlightening. Whether you&rsquo;re concerned about privacy, costs, or just love tinkering, local LLMs are more accessible than ever!</p>
<hr>
<p><em>Want to explore my Ollama setup? Check out the <a href="/pages/projects/ollama">Ollama project documentation</a> or feel free to reach out with questions!</em></p>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
</div>
    <small class="footer_copyright">
        © 2025 .
        
    </small>
</footer>







    
    <script async src="http://localhost:1313/js/main.js" ></script>

    

</body>
</html>
