<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on My New Hugo Site</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Running Local AI: My Experience with Ollama and GPU Acceleration</title>
      <link>http://localhost:1313/posts/local-ai-with-ollama/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/local-ai-with-ollama/</guid>
      
      <description>&lt;p&gt;One of the most exciting aspects of the Fako Cluster is the ability to run large language models locally. With privacy concerns around cloud AI services and the desire for unlimited API usage, I decided to dive deep into self-hosting LLMs using Ollama. Here&amp;rsquo;s what I&amp;rsquo;ve learned.&lt;/p&gt;
&lt;h2 id=&#34;the-hardware-foundation&#34;&gt;The Hardware Foundation&lt;/h2&gt;
&lt;p&gt;Running LLMs locally requires serious GPU power. My setup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: NVIDIA RTX 5070 with 12GB VRAM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Node&lt;/strong&gt;: Dedicated &lt;code&gt;yeezyai&lt;/code&gt; node in the cluster&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 36GB RAM allocated to the Ollama pod&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This configuration allows me to run models up to 32B parameters, though performance varies significantly based on model size.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>GitOps in Practice: Managing Kubernetes with FluxCD</title>
      <link>http://localhost:1313/posts/gitops-with-fluxcd/</link>
      <pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/gitops-with-fluxcd/</guid>
      
      <description>&lt;p&gt;After running the Fako Cluster for several months, I can confidently say that adopting GitOps with FluxCD has been one of the best decisions for managing my Kubernetes infrastructure. In this post, I&amp;rsquo;ll share practical insights and patterns that have proven invaluable.&lt;/p&gt;
&lt;h2 id=&#34;what-is-gitops&#34;&gt;What is GitOps?&lt;/h2&gt;
&lt;p&gt;GitOps is a way of implementing Continuous Deployment for cloud native applications. It focuses on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Declarative Infrastructure&lt;/strong&gt;: Everything defined in Git&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Automated Reconciliation&lt;/strong&gt;: Cluster state matches Git state&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Version Control&lt;/strong&gt;: Complete history of all changes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pull-based Deployment&lt;/strong&gt;: No direct kubectl access needed&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-fluxcd&#34;&gt;Why FluxCD?&lt;/h2&gt;
&lt;p&gt;Among GitOps tools like ArgoCD and Fleet, I chose FluxCD because:&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>Introducing the Fako Cluster: A Personal Kubernetes Infrastructure</title>
      <link>http://localhost:1313/posts/introducing-fako-cluster/</link>
      <pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/introducing-fako-cluster/</guid>
      
      <description>&lt;p&gt;Welcome to the documentation hub for the Fako Cluster! This post marks the beginning of sharing my journey building and maintaining a personal Kubernetes infrastructure that showcases modern cloud-native technologies.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-fako-cluster&#34;&gt;What is the Fako Cluster?&lt;/h2&gt;
&lt;p&gt;The Fako Cluster is a self-hosted Kubernetes environment that I&amp;rsquo;ve built to explore and demonstrate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GitOps practices&lt;/strong&gt; with FluxCD for declarative infrastructure management&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI/ML capabilities&lt;/strong&gt; with local LLM inference using Ollama and GPUStack&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Security-first design&lt;/strong&gt; with external secrets management and RBAC&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comprehensive monitoring&lt;/strong&gt; using Prometheus and Grafana&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modern automation&lt;/strong&gt; with tools like n8n for workflow orchestration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-build-a-personal-cluster&#34;&gt;Why Build a Personal Cluster?&lt;/h2&gt;
&lt;p&gt;As someone driven by sheer curiosity, I wanted a playground where I could:&lt;/p&gt;</description>
      
    </item>
    
  </channel>
</rss>
